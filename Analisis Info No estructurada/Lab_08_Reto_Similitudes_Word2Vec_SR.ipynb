{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# # Práctica de Laboratorio de Procesamiento del Lenguaje Natural (NLP)\n",
    "Tema: Word2Vec\n",
    "\n",
    "\n",
    "Entrenar un modelo de word2vec es una tarea muy costosa computacionalmente, que además requiere de corpus de texto muy grandes, del orden de miles de millones de palabras. Afortunadamente existen modelos word2vec pre-entrenados que están disponibles de forma pública y con los que podemos trabajar para hacer nuestros análisis semánticos. Uno de ellos es el modelo SBWC, disponible aquí: http://cs.famaf.unc.edu.ar/~ccardellino/SBWCE/SBW-vectors-300-min5.bin.gz. Añádelo a la carpeta que contiene este Notebook para poder cargarlo.\n",
    "\n",
    "\n",
    "## Similitud semántica\n",
    "\n",
    "Un modelo pre-entrenado word2vec no es más que un diccionario en el que para cada palabra tenemos el vector que la representa. Podemos cargar en memoria este diccionario utilizando el paquete de análisis de texto gensim."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importamos las librerías\n",
    "\n",
    "import gensim\n",
    "from gensim.models import Word2Vec,KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importamos el modelo Word2Vec\n",
    "\n",
    "model = KeyedVectors.load_word2vec_format('models/SBW-vectors-300-min5.bin.gz',binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos comprobar ahora la representación vectorial de diferentes palabras, por ejemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cómo es un vector\n",
    "\n",
    "vec = model['hombre']\n",
    "print(vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entre otras funcionalidades, el modelo permite localizar aquellos términos más similares al especificado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similitudes\n",
    "\n",
    "model.most_similar('perro',topn=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aunque la representación vectorial de una palabra resulta oscura de interpretar, se ha comprobado que sigue una lógica semántica y sintáctica. Esto permite hacer aritmética con estos vectores y obtener resultados que son coherentes con lo que cabría esperar. Por ejemplo, podemos encontrar casos como: \n",
    "\n",
    "`king` - `man` + `woman` = `queen`\n",
    "\n",
    "Podemos comprobar que eso es cierto utilizando la función most_similar del objeto que contiene los embeddings. Esta función recibe dos listas de palabras, a contribuir de forma positiva o negativa a la operación aritmética, y devuelve las palabras cuya representación vectorial sea más cercana al vector resultado de la operación, ordenadas por similitud. Utiliza el parámetro `positive` para añadir los términos que quieras sumar, y `negative` para restar. \n",
    "\n",
    "### 1. ¿Cuánto es rey + mujer - hombre?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 2. ¿Cuánto es Madrid + Francia - España?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. ¿Cómo conseguir \"hermano\" utilizando \"hermana\"?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Busca al menos 2 ejemplos interesantes para compartir con la clase. ¡Los vamos a comentar!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. ¿Cuál es la similitud entre la palabra perro y la palabra gato? ¿Y entre perro y lápiz?\n",
    "\n",
    "Utiliza la función `similarity` para calcularlo; obtendrás un valor entre 0 y 1, donde 1 indicará la máxima similitud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una utilidad interesante de las distancias semánticas entre palabras es de resolver los típicos pasatiempos en los que se debe identificar la palabra que no encaja dentro de un grupo dado. Por ejemplo:\n",
    "\n",
    "group = [\"Biden\", \"Obama\", \"Bush\", \"camión\"]\n",
    "\n",
    "Está claro que camión es la palabra intrusa en esta lista de presidentes de EEUU. Pero esto es algo que sabemos por nuestro amplio conocimiento del mundo y del lenguaje, y para un programa informático no es nada trivial llegar a esta conclusión. Sin embargo gracias a las representaciones semánticas en forma de vector que nos da word2vec podemos hacerlo.\n",
    "\n",
    "### 5. Utiliza la función `doesnt_match` para comprobar que de la lista: televisión, tostadora, lavavajillas y montaña, montaña es la palabra que no encaja.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 168.34,
   "position": {
    "height": "40px",
    "left": "1225px",
    "right": "20px",
    "top": "36px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "none",
   "window_display": true
  },
  "vscode": {
   "interpreter": {
    "hash": "107fb03afb2754bdb3cdbb13c1c83d7d6037442339c22e5ee8cf40869e8513c5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
