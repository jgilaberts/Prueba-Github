{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Formas canónicas: lematización y stemming\n",
    "\n",
    "NLTK y Spacy son dos librerías que se pueden utilizar para lematizar palabras. En esta práctica se va a realizar una comparativa de ambas librerías para comparar tanto resultados como tiempos de ejecución. Rellena las líneas de código que se piden, y responde a las preguntas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estructura del código\n",
    "1. Importación de librerías y carga de modelo (si es necesario)\n",
    "2. Definición de funciones. En este caso, de una que lea un fichero txt.\n",
    "3. Lectura del fichero.\n",
    "4. `NLTK` Creación del \"objeto\" del modelo de stemming (extracción de lexemas)\n",
    "5. `NLTK` Tokenización\n",
    "6. `NLTK` Aplicación del stemming a cada token con SnowballStemmer\n",
    "7. `Spacy` Creación del \"objeto\" Doc\n",
    "8. `Spacy` Aplicación del lematizador a cada token (palabra.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Importación de librerías. Importa nltk y spacy\n",
    "\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import spacy\n",
    "\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "### carga el modelo es_core_news_lg de spacy\n",
    "nlp = spacy.load(\"es_core_news_lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Genera una función que lea un fichero txt\n",
    "\n",
    "def read_text_file(filename):\n",
    "    file = open(filename, \"r\", encoding=\"utf-8\") \n",
    "    return file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemms - NLTK\n",
      "['pues', 'sep', 'v.m', '.', 'ante', 'tod', 'cos', 'que', 'a', 'mi', 'llam', 'lazar', 'de', 'torm', ',']\n",
      "Lematización - spaCy\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Palabra</th>\n",
       "      <th>F. Canónica</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Pues</td>\n",
       "      <td>pues</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sepa</td>\n",
       "      <td>sepa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>V.M.</td>\n",
       "      <td>V.M.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ante</td>\n",
       "      <td>ante</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>todas</td>\n",
       "      <td>todo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>cosas</td>\n",
       "      <td>cosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>que</td>\n",
       "      <td>que</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>mí</td>\n",
       "      <td>yo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>llaman</td>\n",
       "      <td>llamar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Lázaro</td>\n",
       "      <td>Lázaro</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>de</td>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Tormes</td>\n",
       "      <td>Tormes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>,</td>\n",
       "      <td>,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>hijo</td>\n",
       "      <td>hijo</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Palabra F. Canónica\n",
       "0     Pues        pues\n",
       "1     sepa        sepa\n",
       "2     V.M.        V.M.\n",
       "3     ante        ante\n",
       "4    todas        todo\n",
       "5    cosas        cosa\n",
       "6      que         que\n",
       "7        a           a\n",
       "8       mí          yo\n",
       "9   llaman      llamar\n",
       "10  Lázaro      Lázaro\n",
       "11      de          de\n",
       "12  Tormes      Tormes\n",
       "13       ,           ,\n",
       "14    hijo        hijo"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lee el archivo Lazarillo.txt con la función generada\n",
    "text = read_text_file(\"data/Lazarillo.txt\")\n",
    "\n",
    "# Stemming o extracción de lexemas - NLTK\n",
    "# Se utilizará el modelo SnowballSteemer configurandolo para español para la extracción de raíces\n",
    "print(\"Stemms - NLTK\")\n",
    "spanish_stemmer = SnowballStemmer('spanish')\n",
    "\n",
    "# Genera los tokens del texto. Se recomienda utilizar los recursos mostrados en Lab_06\n",
    "tokens = nltk.word_tokenize(text)\n",
    "\n",
    "# Aplicación de la lematización a cada token\n",
    "stems=[]\n",
    "for token in tokens:\n",
    "    # Aplica el lematizador sobre cada token. Se recomienda utilizar spanish_steemer.stem()\n",
    "    result = spanish_stemmer.stem(token)\n",
    "    # Guardamos el resultado en una lista\n",
    "    stems.append(result)\n",
    "\n",
    "print(stems[:15])\n",
    "\n",
    "### Lematización en spaCy\n",
    "print(\"Lematización - spaCy\")\n",
    "\n",
    "### Construye el Doc de Spacy con el modelo previamente cargado (es_core_news_lg). \n",
    "doc = nlp(text)\n",
    "\n",
    "### Se extrae el lemma de cada palabra\n",
    "lemmas = []\n",
    "for token in doc:\n",
    "    lemmas.append(token.lemma_)\n",
    "\n",
    "print(lemmas)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_tagged = [(palabra.text, palabra.lemma_) for palabra in doc]\n",
    "\n",
    "### Se muestra en una tabla para visualizar correctamente la palabra original y la lematización\n",
    "df=pd.DataFrame(tokens_tagged, columns=['Palabra', 'F. Canónica'])\n",
    "df.iloc[:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio extra. \n",
    "Convierte en función el stemming de NLTK y el lematizador de Spacy, para optimizar el código."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Análisis con spaCy\n",
    "\n",
    "def analyze_spacy(text):\n",
    "    doc = nlp(text)\n",
    "    return(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Stemming - NLTK\n",
    "\n",
    "def stemming_nltk(text):    \n",
    "    spanish_stemmer = SnowballStemmer('spanish')\n",
    "    stems=[]\n",
    "\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    for token in tokens:\n",
    "        stems.append(spanish_stemmer.stem(token)) \n",
    "        \n",
    "    return(stems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  },
  "vscode": {
   "interpreter": {
    "hash": "107fb03afb2754bdb3cdbb13c1c83d7d6037442339c22e5ee8cf40869e8513c5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
