{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Formas canónicas: lematización y stemming\n",
    "\n",
    "NLTK y Spacy son dos librerías que se pueden utilizar para lematizar palabras. En esta práctica se va a realizar una comparativa de ambas librerías para comparar tanto resultados como tiempos de ejecución. Rellena las líneas de código que se piden, y responde a las preguntas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estructura del código\n",
    "1. Importación de librerías y carga de modelo (si es necesario)\n",
    "2. Definición de funciones. En este caso, de una que lea un fichero txt.\n",
    "3. Lectura del fichero.\n",
    "4. `NLTK` Creación del \"objeto\" del modelo de stemming (extracción de lexemas)\n",
    "5. `NLTK` Tokenización\n",
    "6. `NLTK` Aplicación del stemming a cada token con SnowballStemmer\n",
    "7. `Spacy` Creación del \"objeto\" Doc\n",
    "8. `Spacy` Aplicación del lematizador a cada token (palabra.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Importación de librerías. Importa nltk y spacy\n",
    "\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import spacy\n",
    "\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "### carga el modelo es_core_news_lg de spacy\n",
    "nlp = spacy.load(\"es_core_news_lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Genera una función que lea un fichero txt\n",
    "\n",
    "def read_text_file(filename):\n",
    "    file = open(filename, \"r\", encoding=\"utf-8\") \n",
    "    return file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemms - NLTK\n",
      "['pues', 'sep', 'v.m', '.', 'ante', 'tod', 'cos', 'que', 'a', 'mi', 'llam', 'lazar', 'de', 'torm', ',']\n",
      "Lematización - spaCy\n",
      "['pues', 'saber', 'V.M.', 'ante', 'todo', 'cosa', 'que', 'a', 'yo', 'llamar', 'Lázaro', 'de', 'Tormes', ',', 'hijo', 'de', 'Tomé', 'González', 'y', 'de', 'Antona', 'Pérez', ',', 'natural', 'de', 'Tejares', ',', 'aldea', 'de', 'Salamanca', '.', 'mi', 'nacimiento', 'ser', 'dentro', 'del', 'río', 'Tormes', ',', 'por', 'el', 'cual', 'causa', 'tomar', 'el', 'sobrenombre', ',', 'y', 'ser', 'destar', 'manera', '.', 'mi', 'padre', ',', 'que', 'Dios', 'perdone', ',', 'tener', 'cargo', 'de', 'proveer', 'uno', 'molienda', 'de', 'uno', 'aceña', ',', 'que', 'estar', 'ribera', 'de', 'aquel', 'río', ',', 'en', 'el', 'cual', 'ser', 'molinero', 'más', 'de', 'quince', 'año', ';', 'y', 'estar', 'mi', 'madre', 'uno', 'noche', 'en', 'el', 'aceña', ',', 'preñado', 'de', 'yo', ',', 'tomóle', 'el', 'parto', 'y', 'parióme', 'allí', ':', 'de', 'manera', 'que', 'con', 'verdad', 'poder', 'decir', 'nacido', 'en', 'el', 'río', '.', 'pues', 'ser', 'yo', 'niño', 'de', 'ocho', 'año', ',', 'achacar', 'a', 'mi', 'padre', 'cierto', 'sangría', 'mal', 'hecho', 'en', 'el', 'costal', 'de', 'el', 'que', 'allí', 'a', 'moler', 'venir', ',', 'por', 'él', 'que', 'ser', 'preso', ',', 'y', 'confesar', 'y', 'no', 'negar', 'y', 'padecer', 'persecución', 'por', 'justicia', '.', 'esperar', 'en', 'Dios', 'que', 'estar', 'en', 'el', 'Gloria', ',', 'pues', 'el', 'Evangelio', 'él', 'llamar', 'bienaventurado', '.', 'en', 'este', 'tiempo', 'él', 'hacer', 'cierto', 'armada', 'contra', 'moro', ',', 'entre', 'el', 'cual', 'ser', 'mi', 'padre', ',', 'que', 'a', 'el', 'sazón', 'estar', 'desterrado', 'por', 'el', 'desastre', 'ya', 'dicho', ',', 'con', 'cargo', 'de', 'acemilero', 'de', 'uno', 'caballero', 'que', 'allá', 'ser', ',', 'y', 'con', 'su', 'señor', ',', 'como', 'leal', 'criado', ',', 'feneceír', 'su', 'vida', '.']\n"
     ]
    }
   ],
   "source": [
    "# lee el archivo Lazarillo.txt con la función generada\n",
    "text = read_text_file(\"Lazarillo.txt\")\n",
    "\n",
    "# Stemming o extracción de lexemas - NLTK\n",
    "# Se utilizará el modelo SnowballSteemer configurandolo para español para la extracción de raíces\n",
    "print(\"Stemms - NLTK\")\n",
    "spanish_stemmer = SnowballStemmer('spanish')\n",
    "\n",
    "# Genera los tokens del texto. Se recomienda utilizar los recursos mostrados en Lab_06\n",
    "tokens = nltk.word_tokenize(text)\n",
    "\n",
    "# Aplicación de la lematización a cada token\n",
    "stems=[]\n",
    "for token in tokens:\n",
    "    # Aplica el lematizador sobre cada token. Se recomienda utilizar spanish_steemer.stem()\n",
    "    result = spanish_stemmer.stem(token)\n",
    "    # Guardamos el resultado en una lista\n",
    "    stems.append(result)\n",
    "\n",
    "print(stems[:15])\n",
    "\n",
    "### Lematización en spaCy\n",
    "print(\"Lematización - spaCy\")\n",
    "\n",
    "### Construye el Doc de Spacy con el modelo previamente cargado (es_core_news_lg). \n",
    "doc = nlp(text)\n",
    "\n",
    "### Se extrae el lemma de cada palabra\n",
    "lemmas = []\n",
    "for token in doc:\n",
    "    lemmas.append(token.lemma_)\n",
    "\n",
    "print(lemmas)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Palabra</th>\n",
       "      <th>F. Canónica</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Pues</td>\n",
       "      <td>pues</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sepa</td>\n",
       "      <td>saber</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>V.M.</td>\n",
       "      <td>V.M.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ante</td>\n",
       "      <td>ante</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>todas</td>\n",
       "      <td>todo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>cosas</td>\n",
       "      <td>cosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>que</td>\n",
       "      <td>que</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>mí</td>\n",
       "      <td>yo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>llaman</td>\n",
       "      <td>llamar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Lázaro</td>\n",
       "      <td>Lázaro</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>de</td>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Tormes</td>\n",
       "      <td>Tormes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>,</td>\n",
       "      <td>,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>hijo</td>\n",
       "      <td>hijo</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Palabra F. Canónica\n",
       "0     Pues        pues\n",
       "1     sepa       saber\n",
       "2     V.M.        V.M.\n",
       "3     ante        ante\n",
       "4    todas        todo\n",
       "5    cosas        cosa\n",
       "6      que         que\n",
       "7        a           a\n",
       "8       mí          yo\n",
       "9   llaman      llamar\n",
       "10  Lázaro      Lázaro\n",
       "11      de          de\n",
       "12  Tormes      Tormes\n",
       "13       ,           ,\n",
       "14    hijo        hijo"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_tagged = [(palabra.text, palabra.lemma_) for palabra in doc]\n",
    "\n",
    "### Se muestra en una tabla para visualizar correctamente la palabra original y la lematización\n",
    "df=pd.DataFrame(tokens_tagged, columns=['Palabra', 'F. Canónica'])\n",
    "df.iloc[:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio extra. \n",
    "Convierte en función el stemming de NLTK y el lematizador de Spacy, para optimizar el código."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Análisis con spaCy\n",
    "\n",
    "def analyze_spacy(text):\n",
    "    doc = nlp(text)\n",
    "    return(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Stemming - NLTK\n",
    "\n",
    "def stemming_nltk(text):    \n",
    "    spanish_stemmer = SnowballStemmer('spanish')\n",
    "    stems=[]\n",
    "\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    for token in tokens:\n",
    "        stems.append(spanish_stemmer.stem(token)) \n",
    "        \n",
    "    return(stems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pues',\n",
       " 'sep',\n",
       " 'v.m',\n",
       " '.',\n",
       " 'ante',\n",
       " 'tod',\n",
       " 'cos',\n",
       " 'que',\n",
       " 'a',\n",
       " 'mi',\n",
       " 'llam',\n",
       " 'lazar',\n",
       " 'de',\n",
       " 'torm',\n",
       " ',',\n",
       " 'hij',\n",
       " 'de',\n",
       " 'tom',\n",
       " 'gonzalez',\n",
       " 'y',\n",
       " 'de',\n",
       " 'anton',\n",
       " 'perez',\n",
       " ',',\n",
       " 'natural',\n",
       " 'de',\n",
       " 'tejar',\n",
       " ',',\n",
       " 'alde',\n",
       " 'de',\n",
       " 'salamanc',\n",
       " '.',\n",
       " 'mi',\n",
       " 'nacimient',\n",
       " 'fue',\n",
       " 'dentr',\n",
       " 'del',\n",
       " 'rio',\n",
       " 'torm',\n",
       " ',',\n",
       " 'por',\n",
       " 'la',\n",
       " 'cual',\n",
       " 'caus',\n",
       " 'tom',\n",
       " 'el',\n",
       " 'sobrenombr',\n",
       " ',',\n",
       " 'y',\n",
       " 'fue',\n",
       " 'dest',\n",
       " 'maner',\n",
       " '.',\n",
       " 'mi',\n",
       " 'padr',\n",
       " ',',\n",
       " 'que',\n",
       " 'dios',\n",
       " 'perdon',\n",
       " ',',\n",
       " 'ten',\n",
       " 'carg',\n",
       " 'de',\n",
       " 'prov',\n",
       " 'una',\n",
       " 'moliend',\n",
       " 'de',\n",
       " 'una',\n",
       " 'aceñ',\n",
       " ',',\n",
       " 'que',\n",
       " 'esta',\n",
       " 'riber',\n",
       " 'de',\n",
       " 'aquel',\n",
       " 'rio',\n",
       " ',',\n",
       " 'en',\n",
       " 'la',\n",
       " 'cual',\n",
       " 'fue',\n",
       " 'moliner',\n",
       " 'mas',\n",
       " 'de',\n",
       " 'quinc',\n",
       " 'años',\n",
       " ';',\n",
       " 'y',\n",
       " 'estand',\n",
       " 'mi',\n",
       " 'madr',\n",
       " 'una',\n",
       " 'noch',\n",
       " 'en',\n",
       " 'la',\n",
       " 'aceñ',\n",
       " ',',\n",
       " 'preñ',\n",
       " 'de',\n",
       " 'mi',\n",
       " ',',\n",
       " 'tomol',\n",
       " 'el',\n",
       " 'part',\n",
       " 'y',\n",
       " 'pariom',\n",
       " 'alli',\n",
       " ':',\n",
       " 'de',\n",
       " 'maner',\n",
       " 'que',\n",
       " 'con',\n",
       " 'verd',\n",
       " 'pued',\n",
       " 'dec',\n",
       " 'nac',\n",
       " 'en',\n",
       " 'el',\n",
       " 'rio',\n",
       " '.',\n",
       " 'pues',\n",
       " 'siend',\n",
       " 'yo',\n",
       " 'niñ',\n",
       " 'de',\n",
       " 'ocho',\n",
       " 'años',\n",
       " ',',\n",
       " 'achac',\n",
       " 'a',\n",
       " 'mi',\n",
       " 'padr',\n",
       " 'ciert',\n",
       " 'sangr',\n",
       " 'mal',\n",
       " 'hech',\n",
       " 'en',\n",
       " 'los',\n",
       " 'costal',\n",
       " 'de',\n",
       " 'los',\n",
       " 'que',\n",
       " 'alli',\n",
       " 'a',\n",
       " 'mol',\n",
       " 'ven',\n",
       " ',',\n",
       " 'por',\n",
       " 'lo',\n",
       " 'que',\n",
       " 'fue',\n",
       " 'pres',\n",
       " ',',\n",
       " 'y',\n",
       " 'confes',\n",
       " 'y',\n",
       " 'no',\n",
       " 'neg',\n",
       " 'y',\n",
       " 'padec',\n",
       " 'persecu',\n",
       " 'por',\n",
       " 'justici',\n",
       " '.',\n",
       " 'esper',\n",
       " 'en',\n",
       " 'dios',\n",
       " 'que',\n",
       " 'esta',\n",
       " 'en',\n",
       " 'la',\n",
       " 'glori',\n",
       " ',',\n",
       " 'pues',\n",
       " 'el',\n",
       " 'evangeli',\n",
       " 'los',\n",
       " 'llam',\n",
       " 'bienaventur',\n",
       " '.',\n",
       " 'en',\n",
       " 'este',\n",
       " 'tiemp',\n",
       " 'se',\n",
       " 'hiz',\n",
       " 'ciert',\n",
       " 'armad',\n",
       " 'contr',\n",
       " 'mor',\n",
       " ',',\n",
       " 'entre',\n",
       " 'los',\n",
       " 'cual',\n",
       " 'fue',\n",
       " 'mi',\n",
       " 'padr',\n",
       " ',',\n",
       " 'que',\n",
       " 'a',\n",
       " 'la',\n",
       " 'sazon',\n",
       " 'estab',\n",
       " 'desterr',\n",
       " 'por',\n",
       " 'el',\n",
       " 'desastr',\n",
       " 'ya',\n",
       " 'dich',\n",
       " ',',\n",
       " 'con',\n",
       " 'carg',\n",
       " 'de',\n",
       " 'acemiler',\n",
       " 'de',\n",
       " 'un',\n",
       " 'caballer',\n",
       " 'que',\n",
       " 'alla',\n",
       " 'fue',\n",
       " ',',\n",
       " 'y',\n",
       " 'con',\n",
       " 'su',\n",
       " 'señor',\n",
       " ',',\n",
       " 'com',\n",
       " 'leal',\n",
       " 'cri',\n",
       " ',',\n",
       " 'fenec',\n",
       " 'su',\n",
       " 'vid',\n",
       " '.']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemming_nltk(text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  },
  "vscode": {
   "interpreter": {
    "hash": "107fb03afb2754bdb3cdbb13c1c83d7d6037442339c22e5ee8cf40869e8513c5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
