{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actividad 2 Analisis de Informacion No Estructurada"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# - Javier Gilabert Sabater"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Lea el contenido del fichero csv en un DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5567</th>\n",
       "      <td>spam</td>\n",
       "      <td>This is the 2nd time we have tried 2 contact u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5568</th>\n",
       "      <td>ham</td>\n",
       "      <td>Will Ì_ b going to esplanade fr home?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5569</th>\n",
       "      <td>ham</td>\n",
       "      <td>Pity, * was in mood for that. So...any other s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5570</th>\n",
       "      <td>ham</td>\n",
       "      <td>The guy did some bitching but I acted like i'd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5571</th>\n",
       "      <td>ham</td>\n",
       "      <td>Rofl. Its true to its name</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5572 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     label                                               text\n",
       "0      ham  Go until jurong point, crazy.. Available only ...\n",
       "1      ham                      Ok lar... Joking wif u oni...\n",
       "2     spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3      ham  U dun say so early hor... U c already then say...\n",
       "4      ham  Nah I don't think he goes to usf, he lives aro...\n",
       "...    ...                                                ...\n",
       "5567  spam  This is the 2nd time we have tried 2 contact u...\n",
       "5568   ham              Will Ì_ b going to esplanade fr home?\n",
       "5569   ham  Pity, * was in mood for that. So...any other s...\n",
       "5570   ham  The guy did some bitching but I acted like i'd...\n",
       "5571   ham                         Rofl. Its true to its name\n",
       "\n",
       "[5572 rows x 2 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"spam.csv\", sep=',')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Realice el pre-procesamiento que considere necesario. Puede utilizar funciones de la librería NLTK o spaCy, a su voluntad. Recomendamos una escritura modular del código, para poder hacer pruebas posteriormente, y contestar a las preguntas del punto 6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = df.text.to_list()\n",
    "\n",
    "def limpiar_texto(texto):\n",
    "    if not isinstance(texto, str):\n",
    "        raise ValueError(\"El argumento 'texto' debe ser una cadena de texto.\")\n",
    "    \n",
    "    # Eliminar caracteres especiales y números\n",
    "    texto = re.sub(r'[^a-zA-Z\\s]', '', texto)\n",
    "    \n",
    "    # Convertir a minúsculas\n",
    "    texto = texto.lower()\n",
    "    \n",
    "    # Tokenizar el texto\n",
    "    tokens = word_tokenize(texto)\n",
    "    \n",
    "    # Eliminar stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens_filtrados = [word for word in tokens if word not in stop_words]\n",
    "    \n",
    "    # Unir tokens nuevamente en un string\n",
    "    texto_limpiado = ' '.join(tokens_filtrados)\n",
    "    \n",
    "    return texto_limpiado\n",
    "\n",
    "def limpiar_corpus(corpus):\n",
    "    corpus_limpiado = []\n",
    "    \n",
    "    for email in corpus:\n",
    "        try:\n",
    "            email_limpiado = limpiar_texto(email)\n",
    "            corpus_limpiado.append(email_limpiado)\n",
    "        except ValueError as e:\n",
    "            print(f\"Error: {e}\")\n",
    "            corpus_limpiado.append('error')\n",
    "    \n",
    "    return corpus_limpiado\n",
    "\n",
    "# Llamar a la función para limpiar el corpus\n",
    "corpus_limpiado = limpiar_corpus(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>corpus_limpiado</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>go jurong point crazy available bugis n great ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>ok lar joking wif u oni</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>free entry wkly comp win fa cup final tkts st ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>u dun say early hor u c already say</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>nah dont think goes usf lives around though</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5567</th>\n",
       "      <td>spam</td>\n",
       "      <td>This is the 2nd time we have tried 2 contact u...</td>\n",
       "      <td>nd time tried contact u u pound prize claim ea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5568</th>\n",
       "      <td>ham</td>\n",
       "      <td>Will Ì_ b going to esplanade fr home?</td>\n",
       "      <td>b going esplanade fr home</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5569</th>\n",
       "      <td>ham</td>\n",
       "      <td>Pity, * was in mood for that. So...any other s...</td>\n",
       "      <td>pity mood soany suggestions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5570</th>\n",
       "      <td>ham</td>\n",
       "      <td>The guy did some bitching but I acted like i'd...</td>\n",
       "      <td>guy bitching acted like id interested buying s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5571</th>\n",
       "      <td>ham</td>\n",
       "      <td>Rofl. Its true to its name</td>\n",
       "      <td>rofl true name</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5572 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     label                                               text  \\\n",
       "0      ham  Go until jurong point, crazy.. Available only ...   \n",
       "1      ham                      Ok lar... Joking wif u oni...   \n",
       "2     spam  Free entry in 2 a wkly comp to win FA Cup fina...   \n",
       "3      ham  U dun say so early hor... U c already then say...   \n",
       "4      ham  Nah I don't think he goes to usf, he lives aro...   \n",
       "...    ...                                                ...   \n",
       "5567  spam  This is the 2nd time we have tried 2 contact u...   \n",
       "5568   ham              Will Ì_ b going to esplanade fr home?   \n",
       "5569   ham  Pity, * was in mood for that. So...any other s...   \n",
       "5570   ham  The guy did some bitching but I acted like i'd...   \n",
       "5571   ham                         Rofl. Its true to its name   \n",
       "\n",
       "                                        corpus_limpiado  \n",
       "0     go jurong point crazy available bugis n great ...  \n",
       "1                               ok lar joking wif u oni  \n",
       "2     free entry wkly comp win fa cup final tkts st ...  \n",
       "3                   u dun say early hor u c already say  \n",
       "4           nah dont think goes usf lives around though  \n",
       "...                                                 ...  \n",
       "5567  nd time tried contact u u pound prize claim ea...  \n",
       "5568                          b going esplanade fr home  \n",
       "5569                        pity mood soany suggestions  \n",
       "5570  guy bitching acted like id interested buying s...  \n",
       "5571                                     rofl true name  \n",
       "\n",
       "[5572 rows x 3 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['corpus_limpiado'] = df['text'].apply(lambda x: limpiar_texto(x))\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Convierta el corpus de documentos en una matriz TF-idf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aa</th>\n",
       "      <th>aah</th>\n",
       "      <th>aaniye</th>\n",
       "      <th>aaooooright</th>\n",
       "      <th>aathilove</th>\n",
       "      <th>aathiwhere</th>\n",
       "      <th>ab</th>\n",
       "      <th>abbey</th>\n",
       "      <th>abdomen</th>\n",
       "      <th>abeg</th>\n",
       "      <th>...</th>\n",
       "      <th>zeros</th>\n",
       "      <th>zf</th>\n",
       "      <th>zhong</th>\n",
       "      <th>zindgi</th>\n",
       "      <th>zoe</th>\n",
       "      <th>zogtorius</th>\n",
       "      <th>zoom</th>\n",
       "      <th>zouk</th>\n",
       "      <th>zs</th>\n",
       "      <th>zyada</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>go jurong point crazy available bugis n great world la e buffet cine got amore wat</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ok lar joking wif u oni</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>free entry wkly comp win fa cup final tkts st may text fa receive entry questionstd txt ratetcs apply overs</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>u dun say early hor u c already say</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nah dont think goes usf lives around though</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nd time tried contact u u pound prize claim easy call p per minute btnationalrate</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>b going esplanade fr home</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pity mood soany suggestions</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>guy bitching acted like id interested buying something else next week gave us free</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rofl true name</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5572 rows × 8385 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     aa  aah  aaniye  \\\n",
       "go jurong point crazy available bugis n great w...  0.0  0.0     0.0   \n",
       "ok lar joking wif u oni                             0.0  0.0     0.0   \n",
       "free entry wkly comp win fa cup final tkts st m...  0.0  0.0     0.0   \n",
       "u dun say early hor u c already say                 0.0  0.0     0.0   \n",
       "nah dont think goes usf lives around though         0.0  0.0     0.0   \n",
       "...                                                 ...  ...     ...   \n",
       "nd time tried contact u u pound prize claim eas...  0.0  0.0     0.0   \n",
       "b going esplanade fr home                           0.0  0.0     0.0   \n",
       "pity mood soany suggestions                         0.0  0.0     0.0   \n",
       "guy bitching acted like id interested buying so...  0.0  0.0     0.0   \n",
       "rofl true name                                      0.0  0.0     0.0   \n",
       "\n",
       "                                                    aaooooright  aathilove  \\\n",
       "go jurong point crazy available bugis n great w...          0.0        0.0   \n",
       "ok lar joking wif u oni                                     0.0        0.0   \n",
       "free entry wkly comp win fa cup final tkts st m...          0.0        0.0   \n",
       "u dun say early hor u c already say                         0.0        0.0   \n",
       "nah dont think goes usf lives around though                 0.0        0.0   \n",
       "...                                                         ...        ...   \n",
       "nd time tried contact u u pound prize claim eas...          0.0        0.0   \n",
       "b going esplanade fr home                                   0.0        0.0   \n",
       "pity mood soany suggestions                                 0.0        0.0   \n",
       "guy bitching acted like id interested buying so...          0.0        0.0   \n",
       "rofl true name                                              0.0        0.0   \n",
       "\n",
       "                                                    aathiwhere   ab  abbey  \\\n",
       "go jurong point crazy available bugis n great w...         0.0  0.0    0.0   \n",
       "ok lar joking wif u oni                                    0.0  0.0    0.0   \n",
       "free entry wkly comp win fa cup final tkts st m...         0.0  0.0    0.0   \n",
       "u dun say early hor u c already say                        0.0  0.0    0.0   \n",
       "nah dont think goes usf lives around though                0.0  0.0    0.0   \n",
       "...                                                        ...  ...    ...   \n",
       "nd time tried contact u u pound prize claim eas...         0.0  0.0    0.0   \n",
       "b going esplanade fr home                                  0.0  0.0    0.0   \n",
       "pity mood soany suggestions                                0.0  0.0    0.0   \n",
       "guy bitching acted like id interested buying so...         0.0  0.0    0.0   \n",
       "rofl true name                                             0.0  0.0    0.0   \n",
       "\n",
       "                                                    abdomen  abeg  ...  zeros  \\\n",
       "go jurong point crazy available bugis n great w...      0.0   0.0  ...    0.0   \n",
       "ok lar joking wif u oni                                 0.0   0.0  ...    0.0   \n",
       "free entry wkly comp win fa cup final tkts st m...      0.0   0.0  ...    0.0   \n",
       "u dun say early hor u c already say                     0.0   0.0  ...    0.0   \n",
       "nah dont think goes usf lives around though             0.0   0.0  ...    0.0   \n",
       "...                                                     ...   ...  ...    ...   \n",
       "nd time tried contact u u pound prize claim eas...      0.0   0.0  ...    0.0   \n",
       "b going esplanade fr home                               0.0   0.0  ...    0.0   \n",
       "pity mood soany suggestions                             0.0   0.0  ...    0.0   \n",
       "guy bitching acted like id interested buying so...      0.0   0.0  ...    0.0   \n",
       "rofl true name                                          0.0   0.0  ...    0.0   \n",
       "\n",
       "                                                     zf  zhong  zindgi  zoe  \\\n",
       "go jurong point crazy available bugis n great w...  0.0    0.0     0.0  0.0   \n",
       "ok lar joking wif u oni                             0.0    0.0     0.0  0.0   \n",
       "free entry wkly comp win fa cup final tkts st m...  0.0    0.0     0.0  0.0   \n",
       "u dun say early hor u c already say                 0.0    0.0     0.0  0.0   \n",
       "nah dont think goes usf lives around though         0.0    0.0     0.0  0.0   \n",
       "...                                                 ...    ...     ...  ...   \n",
       "nd time tried contact u u pound prize claim eas...  0.0    0.0     0.0  0.0   \n",
       "b going esplanade fr home                           0.0    0.0     0.0  0.0   \n",
       "pity mood soany suggestions                         0.0    0.0     0.0  0.0   \n",
       "guy bitching acted like id interested buying so...  0.0    0.0     0.0  0.0   \n",
       "rofl true name                                      0.0    0.0     0.0  0.0   \n",
       "\n",
       "                                                    zogtorius  zoom  zouk  \\\n",
       "go jurong point crazy available bugis n great w...        0.0   0.0   0.0   \n",
       "ok lar joking wif u oni                                   0.0   0.0   0.0   \n",
       "free entry wkly comp win fa cup final tkts st m...        0.0   0.0   0.0   \n",
       "u dun say early hor u c already say                       0.0   0.0   0.0   \n",
       "nah dont think goes usf lives around though               0.0   0.0   0.0   \n",
       "...                                                       ...   ...   ...   \n",
       "nd time tried contact u u pound prize claim eas...        0.0   0.0   0.0   \n",
       "b going esplanade fr home                                 0.0   0.0   0.0   \n",
       "pity mood soany suggestions                               0.0   0.0   0.0   \n",
       "guy bitching acted like id interested buying so...        0.0   0.0   0.0   \n",
       "rofl true name                                            0.0   0.0   0.0   \n",
       "\n",
       "                                                     zs  zyada  \n",
       "go jurong point crazy available bugis n great w...  0.0    0.0  \n",
       "ok lar joking wif u oni                             0.0    0.0  \n",
       "free entry wkly comp win fa cup final tkts st m...  0.0    0.0  \n",
       "u dun say early hor u c already say                 0.0    0.0  \n",
       "nah dont think goes usf lives around though         0.0    0.0  \n",
       "...                                                 ...    ...  \n",
       "nd time tried contact u u pound prize claim eas...  0.0    0.0  \n",
       "b going esplanade fr home                           0.0    0.0  \n",
       "pity mood soany suggestions                         0.0    0.0  \n",
       "guy bitching acted like id interested buying so...  0.0    0.0  \n",
       "rofl true name                                      0.0    0.0  \n",
       "\n",
       "[5572 rows x 8385 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Crear una instancia de TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Ajustar el vectorizador a los argumentos preprocesados y transformar los datos\n",
    "X_tfidf = tfidf_vectorizer.fit_transform(corpus_limpiado)\n",
    "\n",
    "# Convertir la matriz X_tfidf a un DataFrame de pandas\n",
    "df_tfidf = pd.DataFrame(X_tfidf.toarray(), columns=tfidf_vectorizer.get_feature_names_out(), index=corpus_limpiado)\n",
    "\n",
    "# df_tfidf ahora contiene la representación TF-IDF en formato DataFrame\n",
    "df_tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Divida en un subconjunto de entrenamiento y otro de evaluación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_num = 100\n",
    "df_train, df_test = train_test_split(df, test_size=0.3, random_state=random_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Llegados a este punto, realice modelos de entrenamiento al menos con algoritmos de clasificador bayesiano ingenuo, máquinas SVM y un modelo basado en árbol de decisión. Obtenga resultados de accuracy de la clasificación, así como las matrices de confusión para los tres modelos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clasificador Bayesiano Ingenuo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes:\n",
      "Accuracy: 0.97\n",
      "\n",
      "Confusion Matrix:\n",
      "[[1459    0]\n",
      " [  56  157]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham       0.96      1.00      0.98      1459\n",
      "        spam       1.00      0.74      0.85       213\n",
      "\n",
      "    accuracy                           0.97      1672\n",
      "   macro avg       0.98      0.87      0.91      1672\n",
      "weighted avg       0.97      0.97      0.96      1672\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Crear el modelo usando un pipeline con TF-IDF y el clasificador Naive Bayes o Bayesiano Ingenuo\n",
    "model = make_pipeline(TfidfVectorizer(), MultinomialNB())\n",
    "\n",
    "# Entrenar el modelo con el conjunto de entrenamiento\n",
    "model.fit(df_train['corpus_limpiado'], df_train['label'])\n",
    "\n",
    "# Realizar predicciones en el conjunto de evaluación\n",
    "predictions = model.predict(df_test['corpus_limpiado'])\n",
    "\n",
    "# Evaluar el rendimiento del modelo\n",
    "accuracy = accuracy_score(df_test['label'], predictions)\n",
    "conf_matrix = confusion_matrix(df_test['label'], predictions)\n",
    "classification_rep = classification_report(df_test['label'], predictions)\n",
    "\n",
    "# Mostrar los resultados\n",
    "print(\"Naive Bayes:\")\n",
    "print(f'Accuracy: {accuracy:.2f}')\n",
    "print('\\nConfusion Matrix:') \n",
    "print(conf_matrix)\n",
    "print('\\nClassification Report:')\n",
    "print(classification_rep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Con este modelo se ha alcanzado una alta precisión del 97%, con un alto recall para los mensajes NO spam (ham) del 100%, pero con un bajo recall para los mensajes spam del 74%.\n",
    "- En la matriz de confusion podemos observar 56 (FN) Falsos Negativos que fueron identificados como ham, siendo realmente spam."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maquinas SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maquinas SVM:\n",
      "Accuracy: 0.98\n",
      "\n",
      "Confusion Matrix:\n",
      "[[1455    4]\n",
      " [  35  178]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham       0.98      1.00      0.99      1459\n",
      "        spam       0.98      0.84      0.90       213\n",
      "\n",
      "    accuracy                           0.98      1672\n",
      "   macro avg       0.98      0.92      0.94      1672\n",
      "weighted avg       0.98      0.98      0.98      1672\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Crear el modelo usando un pipeline con TF-IDF y el clasificador Naive Bayes o Bayesiano Ingenuo\n",
    "svm_model = make_pipeline(TfidfVectorizer(), SVC())\n",
    "\n",
    "# Entrenar el modelo con el conjunto de entrenamiento\n",
    "svm_model.fit(df_train['corpus_limpiado'], df_train['label'])\n",
    "\n",
    "# Realizar predicciones en el conjunto de evaluación\n",
    "svm_predictions = svm_model.predict(df_test['corpus_limpiado'])\n",
    "\n",
    "# Evaluar el rendimiento del modelo\n",
    "svm_accuracy = accuracy_score(df_test['label'], svm_predictions)\n",
    "svm_conf_matrix = confusion_matrix(df_test['label'], svm_predictions)\n",
    "svm_classification_rep = classification_report(df_test['label'], svm_predictions)\n",
    "\n",
    "# Mostrar los resultados\n",
    "print('Maquinas SVM:')\n",
    "print(f'Accuracy: {svm_accuracy:.2f}')\n",
    "print('\\nConfusion Matrix:') \n",
    "print(svm_conf_matrix)\n",
    "print('\\nClassification Report:')\n",
    "print(svm_classification_rep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Con este modelo se ha alcanzado una precisión aun mayor que con el modelo anterior del 98%, con un alto recall para los mensajes NO spam (ham) del 100%, pero con un bajo recall para los mensajes spam, aunque mayor que con el bayesiano del 84%.\n",
    "- En la matriz de confusion podemos observar 35 (FN) Falsos Negativos que fueron identificados como ham, siendo realmente spam. Y 4 (FP) Falsos Positivos que fueron identificados como spam, siendo realmente ham."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Arboles de Decision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arbol de Decision:\n",
      "Accuracy: 0.96\n",
      "\n",
      "Confusion Matrix:\n",
      "[[1434   25]\n",
      " [  40  173]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham       0.97      0.98      0.98      1459\n",
      "        spam       0.87      0.81      0.84       213\n",
      "\n",
      "    accuracy                           0.96      1672\n",
      "   macro avg       0.92      0.90      0.91      1672\n",
      "weighted avg       0.96      0.96      0.96      1672\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Crear el modelo usando un pipeline con TF-IDF y el clasificador Naive Bayes o Bayesiano Ingenuo\n",
    "tree_model = make_pipeline(TfidfVectorizer(), DecisionTreeClassifier(random_state=random_num))\n",
    "\n",
    "# Entrenar el modelo con el conjunto de entrenamiento\n",
    "tree_model.fit(df_train['corpus_limpiado'], df_train['label'])\n",
    "\n",
    "# Realizar predicciones en el conjunto de evaluación\n",
    "tree_predictions = tree_model.predict(df_test['corpus_limpiado'])\n",
    "\n",
    "# Evaluar el rendimiento del modelo\n",
    "tree_accuracy = accuracy_score(df_test['label'], tree_predictions)\n",
    "tree_conf_matrix = confusion_matrix(df_test['label'], tree_predictions)\n",
    "tree_classification_rep = classification_report(df_test['label'], tree_predictions)\n",
    "\n",
    "# Mostrar los resultados\n",
    "print('Arbol de Decision:')\n",
    "print(f'Accuracy: {tree_accuracy:.2f}')\n",
    "print('\\nConfusion Matrix:') \n",
    "print(tree_conf_matrix)\n",
    "print('\\nClassification Report:')\n",
    "print(tree_classification_rep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Con este modelo se ha alcanzado una precisión alta aunque la mas baja de todos los modelos del 96%, con un alto recall para los mensajes NO spam (ham) del 98%, mas bajo que cualquier modelo, pero con un bajo recall para los mensajes spam del 81%, mas bajo que el modelo SVM.\n",
    "- En la matriz de confusion podemos observar 40 (FN) Falsos Negativos que fueron identificados como ham, siendo realmente spam. Y 25 (FP) Falsos Positivos que fueron identificados como spam, siendo realmente ham."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### En conclusion, comparando los tres modelos, podemos decir que el mejor modelo de todos es el SVM, al ser el mas preciso de los tres en todos los ambitos, con mas precision y recall que cualquiera de los otros modelos. Aunque si que hay que tener en cuenta que el bayesiano no tenia falsos positivos, mientras que el svm tiene 4 FP, aunque la cantidad de FN tambien la tiene el SVM, asi que este seria el modelo a escoger."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Conteste a las siguientes preguntas basándote en evidencias de código. ¿Tiene influencia en el resultado final el número máximo de features a utilizar? Prueba al menos dos números de features diferentes para los tres algoritmos y mide los resultados. ¿Modifica el resultado si no se eliminan las stop words? Pruébalo para los tres algoritmos y mide los resultados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - Influencia del numero maximo de features a utilizar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clasificador Bayesiano Ingenuo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Features: 100, Accuracy: 0.95\n",
      "Max Features: 1000, Accuracy: 0.98\n"
     ]
    }
   ],
   "source": [
    "max_features_values = [100, 1000]\n",
    "\n",
    "for max_features in max_features_values:\n",
    "    model = make_pipeline(TfidfVectorizer(max_features=max_features), MultinomialNB())\n",
    "\n",
    "    model.fit(df_train['corpus_limpiado'], df_train['label'])\n",
    "\n",
    "    predictions = model.predict(df_test['corpus_limpiado'])\n",
    "\n",
    "    accuracy = accuracy_score(df_test['label'], predictions)\n",
    "    print(f'Max Features: {max_features}, Accuracy: {accuracy:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maquinas SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Features: 100, Accuracy: 0.96\n",
      "Max Features: 1000, Accuracy: 0.98\n"
     ]
    }
   ],
   "source": [
    "max_features_values = [100, 1000]\n",
    "\n",
    "for max_features in max_features_values:\n",
    "    model = make_pipeline(TfidfVectorizer(max_features=max_features), SVC())\n",
    "\n",
    "    model.fit(df_train['corpus_limpiado'], df_train['label'])\n",
    "\n",
    "    predictions = model.predict(df_test['corpus_limpiado'])\n",
    "\n",
    "    accuracy = accuracy_score(df_test['label'], predictions)\n",
    "    print(f'Max Features: {max_features}, Accuracy: {accuracy:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Arboles de Decision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Features: 100, Accuracy: 0.94\n",
      "Max Features: 1000, Accuracy: 0.96\n"
     ]
    }
   ],
   "source": [
    "max_features_values = [100, 1000]\n",
    "\n",
    "for max_features in max_features_values:\n",
    "    model = make_pipeline(TfidfVectorizer(max_features=max_features), DecisionTreeClassifier(random_state=random_num))\n",
    "\n",
    "    model.fit(df_train['corpus_limpiado'], df_train['label'])\n",
    "\n",
    "    predictions = model.predict(df_test['corpus_limpiado'])\n",
    "\n",
    "    accuracy = accuracy_score(df_test['label'], predictions)\n",
    "    print(f'Max Features: {max_features}, Accuracy: {accuracy:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### En cuanto al numero de features podemos ver que afecta 100%, ya que para los tres modelos una mayor cantidad de features incrementa el accuracy, para 100 features es mas bajo en todos los casos que con 1000 features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - Influencia de no usar stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clasificador Bayesiano Ingenuo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No stopwords Accuracy: 0.96\n"
     ]
    }
   ],
   "source": [
    "model = make_pipeline(TfidfVectorizer(), MultinomialNB())\n",
    "\n",
    "model.fit(df_train['text'], df_train['label'])\n",
    "\n",
    "predictions = model.predict(df_test['text'])\n",
    "\n",
    "accuracy = accuracy_score(df_test['label'], predictions)\n",
    "print(f'No stopwords Accuracy: {accuracy:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maquina SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No stopwords Accuracy: 0.98\n"
     ]
    }
   ],
   "source": [
    "model = make_pipeline(TfidfVectorizer(), SVC())\n",
    "\n",
    "model.fit(df_train['text'], df_train['label'])\n",
    "\n",
    "predictions = model.predict(df_test['text'])\n",
    "\n",
    "accuracy = accuracy_score(df_test['label'], predictions)\n",
    "print(f'No stopwords Accuracy: {accuracy:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Arboles de Decision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No stopwords Accuracy: 0.96\n"
     ]
    }
   ],
   "source": [
    "model = make_pipeline(TfidfVectorizer(), DecisionTreeClassifier())\n",
    "\n",
    "model.fit(df_train['text'], df_train['label'])\n",
    "\n",
    "predictions = model.predict(df_test['text'])\n",
    "\n",
    "accuracy = accuracy_score(df_test['label'], predictions)\n",
    "print(f'No stopwords Accuracy: {accuracy:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### En cuanto a las stopwords podemos observar que al unico modelo al que le afecta el uso de stopwords es al bayesiano, ya que en el ejercicio 5 hemos conseguido una precision del 97% y aqui en cambio tenemos una precision del 96%. Pero en terminos general no parace afectar al mismo nivel que el numero de features que si que afecta mas la precision, ya que para el SVM y el arbol de decision nos sale la misma precision tanto con stopwords como sin stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Imagínese que este entregable es una labor que le han solicitado en un entorno profesional, y que tiene que entregar esta documentación para comentar lo que ha descubierto (datos de entrada, rendimiento de los modelos, o cualquier descubrimiento que pueda ser importante). Comente los resultados obtenidos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mirando los datos de entrada encontramos dos columnas, \"text\" incluyendo el texto de todos los emails y \"label\" indicando si el email es spam o no (ham). En cuanto al rendimiento de los modelos, se han probado tres modelos diferentes, un clasificador bayesiano ingenuo, una maquina de soporte vectorial (SVM) y un arbol de decision, adjudicando para esta tarea finalmente la maquina de soporte vectorial (SVM) debido a una mayor precision y un mayor recall. Finalmente hemos hecho unas pruebas para comprobar si el numero de features y el uso de stopwords alteraba de alguna manera la seleccion del modelo a usar. Con lo que hemos concluido que el numero de features si que afecta a la precision del modelo y el uso de stopwords solo afecta al modelo bayesiano. Como conclusion final, hemos conseguido una precision del 98% para identificar que emails son spam con el uso del modelo SVM, lo cual es una precision alta."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
