{
 "cells": [
  {
   "attachments": {
    "CUNEFlogo.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![CUNEFlogo.png](attachment:CUNEFlogo.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#  <font color=orange> Notebook: Shallow Learning en Series Temporales</font>\n",
    "## Máster de Ciencia de Datos\n",
    "### Modelos de Inteligencia Artificial\n",
    "\n",
    "<b> Profesor: </b> Wolfram Rozas. <b> Departamento: </b> Métodos Cuantitativos\n",
    "___________________________________________________________________________________________\n",
    "__Resumen__\n",
    "\n",
    "\n",
    "Este *cuaderno* describe algunos métodos de regresión de aprendizaje superficial aplicados a las series temporales de consumo y producción de energía (medida en kWh) y % de margen libre de batería (headroom%) con datos de granularidad diaria.\n",
    "\n",
    "Se aplican modelos de regresión de paquetes Scikit-learn para realizar pronósticos en series temporales. Específicamente, utilizamos SKForecast (*), una biblioteca simple que contiene las clases y funciones necesarias para adaptar cualquier modelo de regresión de Scikit-learn a problemas de pronóstico.\n",
    "\n",
    "Los modelos de pronóstico autorregresivo recursivo con variables exógenas se desarrollan con las siguientes técnicas de regresión:\n",
    "<ol>\n",
    "    <li>Regresión lineal</li>\n",
    "    <li>Lasso</li>\n",
    "    <li>Regresión Cresta</li>\n",
    "    <li>Regresión Cresta Bayesiana</li>\n",
    "    <li>Bosque Aleatorio</li>\n",
    "    <li>Aumento de Gradiente Extremo (XGB)</li>\n",
    "</ol>\n",
    "\n",
    "(*) Rodrigo, J.A. Escobar Ortiz, J. _Skforecast: pronóstico de series de tiempo con Python y Scikit-learn_, disponible bajo Atribución 4.0 Internacional (CC BY 4.0) en https://www.cienciadedatos.net/documentos/py27-time-series-forecasting-python -scikitlearn.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Regression models in Time Series\n",
    "\n",
    "In order to apply machine learning models to forecasting problems, the time series has to be transformed into a matrix in which each value is related to the time window (lags) that precedes it. This is called the _sliding window_\n",
    "\n",
    "In a time series context, a lag with respect to a time step  t  is defined as the values of the series at previous time steps. For example, lag 1 is the value at time step  t−1  and lag  m  is the value at time step  t−m .\n",
    "\n",
    "\n",
    "<br>\n",
    " \n",
    "<div>\n",
    "<img src=\"attachment:transform_timeseries.gif\" width=\"500\"/>\n",
    "</div>\n",
    "\n",
    "Time series transformation into a matrix of 5 lags and a vector with the value of the series that follows each row of the matrix.\n",
    "\n",
    "This type of transformation also allows to include additional variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"attachment:matrix_transformation_with_exog_variable.png\" width=\"500\"/>\n",
    "</div>\n",
    "\n",
    "Time series transformation including an exogenous variable.\n",
    "Once data have been rearranged into the new shape, any regression model can be trained to predict the next value (step) of the series. During model training, every row is considered a separate data instance, where values at lags 1, 2, ...  p  are considered predictors for the target quantity of the time series at time step  p+1. SKForecast package builds the dependent variable lags, but not the exogenous variable ones that should be computed during the computation process.\n",
    "\n",
    "The main complexity of this approach is to generate the correct training matrices for each model, _the sliding window_\n",
    "\n",
    "<div>\n",
    "<img src=\"attachment:diagram_skforecast_multioutput.png\" width=\"700\"/>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once data have been rearranged into the new shape, any regression model can be trained to predict the next value (step) of the series. During model training, every row is considered a separate data instance, where values at lags 1, 2, ... p are considered predictors for the target quantity of the time series at time step p+1.\n",
    "\n",
    "<div>\n",
    "<img src=\"attachment:diagram-trainig-forecaster.png\" width=\"500\"/>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Packages\n",
    "\n",
    "# Data manipulation\n",
    "# ==============================================================================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Plots\n",
    "# ==============================================================================\n",
    "import matplotlib.pyplot as plt\n",
    "from cycler import cycler\n",
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "from statsmodels.graphics.tsaplots import plot_pacf\n",
    "%matplotlib inline\n",
    "\n",
    "# Color Palette\n",
    "\n",
    "dark_green = '#00523e'\n",
    "brown = '#2d2572'\n",
    "violet = '#2d2572'\n",
    "pale_red = '#d42f01'\n",
    "dark_blue = '#2d2d8a'\n",
    "turquoise = '#059899'\n",
    "pale_green = '#99cb02'\n",
    "gray = '#808080'\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "# Modeling and Forecasting\n",
    "# ==============================================================================\n",
    "import sklearn as sk\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import BayesianRidge\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error, median_absolute_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "from skforecast.ForecasterAutoreg import ForecasterAutoreg\n",
    "from skforecast.ForecasterAutoregCustom import ForecasterAutoregCustom\n",
    "from skforecast.ForecasterAutoregDirect import ForecasterAutoregDirect\n",
    "from skforecast.model_selection import grid_search_forecaster\n",
    "from skforecast.model_selection import backtesting_forecaster\n",
    "from skforecast.utils import save_forecaster\n",
    "from skforecast.utils import load_forecaster\n",
    "\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Time & DataTime\n",
    "# ==============================================================================\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "# Warnings configuration\n",
    "# ==============================================================================\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEFINIR los directorios para datos, figuras y modelos\n",
    "# actualiza el valor de la variable con un valor alfanúmerico con el directorio\n",
    "\n",
    "#dataset_path = '/home/wrozas/1.0 Dataset/' # TESLA \n",
    "#dataset_path = '/home/wrozas/Insync/crozas2@alumno.uned.es/OneDrive Biz/3 Doctorado/0B. Papers & Thesis/00. Cornwall Dataset LEM Data & Analysis/1.0 Dataset/'# UBUNTU\n",
    "dataset_path = '/workspaces/Prueba-Github/Modelos de Inteligencia Artificial/ACT6/'\n",
    "\n",
    "#figures_path = '/home/wrozas/2. Jupyter Notebooks/00. Cornwall LEM Notebooks/3. SLM/figures/' # TESLA \n",
    "#figures_path = '/home/wrozas/Insync/crozas2@alumno.uned.es/OneDrive Biz/3 Doctorado/0B. Papers & Thesis/00. Cornwall Dataset LEM Data & Analysis/2. Jupyter Notebooks/3. SLM/0. figures/'# UBUNTU\n",
    "figures_path = '/workspaces/Prueba-Github/Modelos de Inteligencia Artificial/ACT6/figures/'\n",
    "\n",
    "#models_path = '/home/wrozas/2. Jupyter Notebooks/00. Cornwall LEM Notebooks/3. SLM/models/' # TESLA \n",
    "#models_path = '/home/wrozas/Insync/crozas2@alumno.uned.es/OneDrive Biz/3 Doctorado/0B. Papers & Thesis/00. Cornwall Dataset LEM Data & Analysis/2. Jupyter Notebooks/3. SLM/3. models/'# UBUNTU\n",
    "models_path = '/workspaces/Prueba-Github/Modelos de Inteligencia Artificial/ACT6/models/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get initial time for this notebook\n",
    "time_init = time.localtime()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".B03 Consumption kWh\n"
     ]
    }
   ],
   "source": [
    "dependent_variable = '1'\n",
    "dependent_variable = input(\"Dependent Variable: 1 = Consumption kW, 2=Production kW, 3 = Headroom% \" )\n",
    "\n",
    "if dependent_variable == '1':\n",
    "    dependent_variable = \".B03 Consumption kWh\"\n",
    "elif dependent_variable == '2':\n",
    "    dependent_variable = \".B04 Production kWh\"\n",
    "else:\n",
    "    dependent_variable = \".B15 Headroom\"\n",
    "\n",
    "print(dependent_variable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression\n"
     ]
    }
   ],
   "source": [
    "selected_regressor = '1'\n",
    "selected_regressor = input(\"Select Regressor: 1 = Linear Regression, 2 = Lasso, 3 = Ridge Regression, 4 = Bayesian Ridge Regression, 5 = Random Forest, 6 = XGB \" )\n",
    "alpha = 0\n",
    "if selected_regressor == '1':\n",
    "    regressor_named = \"Linear Regression\"\n",
    "    \n",
    "elif selected_regressor == '2':\n",
    "\n",
    "    alpha = input(\"alpha: \")\n",
    "    if alpha == \"\":\n",
    "        alpha = 0.1\n",
    "    regressor_named = \"Lasso\"\n",
    "    \n",
    "elif selected_regressor == '3':\n",
    "\n",
    "    alpha = input(\"alpha: \")\n",
    "    if alpha == \"\":\n",
    "        alpha = 0.5\n",
    "    regressor_named = \"Ridge Regression\"\n",
    "    \n",
    "elif selected_regressor == '4':\n",
    "    regressor_named = \"Bayesian Ridge Regression\"\n",
    "    \n",
    "elif selected_regressor == '5':\n",
    "    regressor_named = \"Random Forest Regressor\"\n",
    "    \n",
    "else:\n",
    "    regressor_named = \"XGBoost\"\n",
    "\n",
    "if alpha != 0:\n",
    "    print(regressor_named + \" (alpha = \" + str(alpha) + \")\")\n",
    "else:\n",
    "    print(regressor_named)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Cornwall LEM Hourly Data Loading and Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_multivar = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read t_msb1m_sites_metadata_date_cluster_meteo table\n",
    "# This table is the cross of KPIs by MySonnenBatterie (t_msb1m)at minute granularity level with t_sites (information on BESS and metadata information \n",
    "# (information about the UNIT, BESS, APPLIANCES, DER, EV, BILL ), having the cluster label, and cross with weather information (t_weatherforecasts)\n",
    "# Information is agggregated at hour level having the minute information and filtered in the annual period 30.4.2019-31.3.2020\n",
    "\n",
    "\n",
    "df_multivar = pd.read_csv(dataset_path+'t_msb1m_sites_metadata_date_cluster_meteo.csv', delimiter=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanse NaN values\n",
    "df_multivar.fillna(axis = 1, value= 0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing unnecessary columns\n",
    "\n",
    "df_multivar.drop(columns=df_multivar.columns[2:22], inplace=True)\n",
    "df_multivar.drop(columns=df_multivar.columns[17:71], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert site into integer, date into string and rename it\n",
    "\n",
    "df_multivar['01.01 UNIT site']=df_multivar['01.01 UNIT site'].astype(int)\n",
    "df_multivar['date']=df_multivar['date'].astype(str)\n",
    "\n",
    "# Convert object series in float\n",
    "\n",
    "for i in range(2, 23):\n",
    "    df_multivar[df_multivar.columns[i]]=df_multivar[df_multivar.columns[i]].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 31955 entries, 0 to 31954\n",
      "Data columns (total 23 columns):\n",
      " #   Column                          Non-Null Count  Dtype  \n",
      "---  ------                          --------------  -----  \n",
      " 0   01.01 UNIT site                 31955 non-null  int64  \n",
      " 1   date                            31955 non-null  object \n",
      " 2   .B01 Discharge kWh              31955 non-null  float64\n",
      " 3   .B02 Charge kWh                 31955 non-null  float64\n",
      " 4   .B03 Consumption kWh            31955 non-null  float64\n",
      " 5   .B04 Production kWh             31955 non-null  float64\n",
      " 6   .B05 Grid Export kWh            31955 non-null  float64\n",
      " 7   .B06 Grid Import kWh            31955 non-null  float64\n",
      " 8   .B07 PV Charge kWh              31955 non-null  float64\n",
      " 9   .B08 PV Consumption kWh         31955 non-null  float64\n",
      " 10  .B09 PV Export kWh              31955 non-null  float64\n",
      " 11  .B10 Grid Discharge kWh         31955 non-null  float64\n",
      " 12  .B11 Grid Charge kWh            31955 non-null  float64\n",
      " 13  .B12 Grid Consumption kWh       31955 non-null  float64\n",
      " 14  .B13 Consumption Discharge kWh  31955 non-null  float64\n",
      " 15  .B14 SOC                        31955 non-null  float64\n",
      " 16  .B15 Headroom                   31955 non-null  float64\n",
      " 17  precipitation_Mean              31955 non-null  float64\n",
      " 18  precipitation_probability_Mean  31955 non-null  float64\n",
      " 19  wind_direction_Mean             31955 non-null  float64\n",
      " 20  wind_speed_Mean                 31955 non-null  float64\n",
      " 21  solar_radiation_Mean            31955 non-null  float64\n",
      " 22  sunshine_duration_Mean          31955 non-null  float64\n",
      "dtypes: float64(21), int64(1), object(1)\n",
      "memory usage: 5.6+ MB\n"
     ]
    }
   ],
   "source": [
    "df_multivar.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Dataset Variable Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "#iloc cluster o 01.01 UNIT size\n",
    "\n",
    "\n",
    "# Aggregate by date\n",
    "df_multivar = df_multivar.groupby(['date'], dropna=False).mean()\n",
    "\n",
    "\n",
    "# Drop unnecessary columns\n",
    "df_multivar = df_multivar.drop(['01.01 UNIT site'], axis=1)\n",
    "df_multivar = df_multivar.drop(['.B14 SOC'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_multivar = df_multivar.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 357 entries, 0 to 356\n",
      "Data columns (total 20 columns):\n",
      " #   Column                          Non-Null Count  Dtype  \n",
      "---  ------                          --------------  -----  \n",
      " 0   .B01 Discharge kWh              357 non-null    float64\n",
      " 1   .B02 Charge kWh                 357 non-null    float64\n",
      " 2   .B03 Consumption kWh            357 non-null    float64\n",
      " 3   .B04 Production kWh             357 non-null    float64\n",
      " 4   .B05 Grid Export kWh            357 non-null    float64\n",
      " 5   .B06 Grid Import kWh            357 non-null    float64\n",
      " 6   .B07 PV Charge kWh              357 non-null    float64\n",
      " 7   .B08 PV Consumption kWh         357 non-null    float64\n",
      " 8   .B09 PV Export kWh              357 non-null    float64\n",
      " 9   .B10 Grid Discharge kWh         357 non-null    float64\n",
      " 10  .B11 Grid Charge kWh            357 non-null    float64\n",
      " 11  .B12 Grid Consumption kWh       357 non-null    float64\n",
      " 12  .B13 Consumption Discharge kWh  357 non-null    float64\n",
      " 13  .B15 Headroom                   357 non-null    float64\n",
      " 14  precipitation_Mean              357 non-null    float64\n",
      " 15  precipitation_probability_Mean  357 non-null    float64\n",
      " 16  wind_direction_Mean             357 non-null    float64\n",
      " 17  wind_speed_Mean                 357 non-null    float64\n",
      " 18  solar_radiation_Mean            357 non-null    float64\n",
      " 19  sunshine_duration_Mean          357 non-null    float64\n",
      "dtypes: float64(20)\n",
      "memory usage: 55.9 KB\n"
     ]
    }
   ],
   "source": [
    "df_multivar.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check missing values\n",
    "\n",
    "print(f'Number of rows with missing values: {df_multivar.isnull().values.ravel().sum()}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Data Partitioning: Training and Testing Subsets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset is partitioned in three subsets: one for training with 70% sample size, the second for testing with about 30% sample size. The third portion is for prediction and will have the same number of elements of the output window (steps). Our output window (steps) is 7 days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = 7  # forecasting horizon as output window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lagged_variables = [\n",
    " '.B01 Discharge kWh',\n",
    " '.B02 Charge kWh',\n",
    " '.B03 Consumption kWh',\n",
    " '.B04 Production kWh',\n",
    " '.B05 Grid Export kWh',\n",
    " '.B06 Grid Import kWh',\n",
    " '.B07 PV Charge kWh',\n",
    " '.B08 PV Consumption kWh',\n",
    " '.B09 PV Export kWh',\n",
    " '.B10 Grid Discharge kWh',\n",
    " '.B11 Grid Charge kWh',\n",
    " '.B12 Grid Consumption kWh',\n",
    " '.B13 Consumption Discharge kWh',\n",
    " '.B15 Headroom',\n",
    " 'precipitation_Mean',\n",
    " 'precipitation_probability_Mean',\n",
    " 'wind_direction_Mean',\n",
    " 'wind_speed_Mean',\n",
    " 'solar_radiation_Mean',\n",
    " 'sunshine_duration_Mean'\n",
    " ]  \n",
    "\n",
    "# Creation of Lagged Exogenous\n",
    "\n",
    "for i in range(0, len(lagged_variables)):\n",
    "    for j in range(1, steps+1):\n",
    "        df_multivar[lagged_variables[i]+'_lag_' + str(j)] = df_multivar[lagged_variables[i]].shift(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing exogenous at time t \n",
    "\n",
    "drop_cols = ['.B01 Discharge kWh',\n",
    " '.B02 Charge kWh',\n",
    " '.B05 Grid Export kWh',\n",
    " '.B06 Grid Import kWh',\n",
    " '.B07 PV Charge kWh',\n",
    " '.B08 PV Consumption kWh',\n",
    " '.B09 PV Export kWh',\n",
    " '.B10 Grid Discharge kWh',\n",
    " '.B11 Grid Charge kWh',\n",
    " '.B12 Grid Consumption kWh',\n",
    " '.B13 Consumption Discharge kWh',\n",
    " 'precipitation_Mean',\n",
    " 'precipitation_probability_Mean',\n",
    " 'wind_direction_Mean',\n",
    " 'wind_speed_Mean',\n",
    " 'solar_radiation_Mean',\n",
    " 'sunshine_duration_Mean']\n",
    "\n",
    "if dependent_variable == '.B03 Consumption kWh':\n",
    "    \n",
    "    drop_cols.append('.B04 Production kWh')\n",
    "    drop_cols.append('.B15 Headroom')\n",
    "\n",
    "elif dependent_variable == '.B04 Production kWh':\n",
    "\n",
    "    drop_cols.append('.B03 Consumption kWh')\n",
    "    drop_cols.append('.B15 Headroom')\n",
    "  \n",
    "else:\n",
    "    \n",
    "    drop_cols.append('.B04 Production kWh')\n",
    "    drop_cols.append('.B03 Consumption kWh')\n",
    "\n",
    "df_multivar = df_multivar.drop(drop_cols, axis=1)\n",
    "cols_df_multivar = df_multivar.columns.to_list()\n",
    "exog_variables  = cols_df_multivar[1:len(cols_df_multivar)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanse NaN values\n",
    "df_multivar = df_multivar.fillna(df_multivar.median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_multivar.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating train-test-predict subsets \n",
    "# ==============================================================================\n",
    "\n",
    "partition = round(len(df_multivar)*0.7) # 70% training subset\n",
    "test_size = len(df_multivar) - partition # 30% testing subset\n",
    "\n",
    "data_train = df_multivar[:partition] # training subset\n",
    "data_test  = df_multivar[partition:len( df_multivar)-steps] # testing subset = 30% - steps\n",
    "data_predict = df_multivar[len( df_multivar)-steps:] # predicting subset for exogenous variables (steps)\n",
    "\n",
    "print(f\"Train dates : {data_train.index.min()} --- {data_train.index.max()}  (n={len(data_train)})\")\n",
    "print(f\"Test dates  : {data_test.index.min()} --- {data_test.index.max()}  (n={len(data_test)})\")\n",
    "print(f\"Total dataset size  (n={ df_multivar.shape[0] })\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanse NaN values\n",
    "data_train.fillna(axis = 1, value= 0, inplace=True)\n",
    "data_test.fillna(axis = 1, value= 0, inplace=True)\n",
    "data_predict.fillna(axis = 1, value= 0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Training and Testing Dependent Variable\n",
    "\n",
    "plt.figure(figsize=(30, 10))\n",
    "\n",
    "x_train_axis = data_train.index\n",
    "x_test_axis = data_test.index\n",
    "\n",
    "x_axis = np.arange(1, len(data_train.index)+1,1)\n",
    "y_axis = np.arange(len(data_train.index),len(data_train.index)+len(data_test.index),1)\n",
    "\n",
    "# EN VEZ DE AVERAGE SITE PONER CLUSTER 4 O SITE 100\n",
    "plt.title(dependent_variable + ' Daily Training & Testing Subsets (Average Site)', fontsize=30, loc = 'left', fontweight=\"bold\")\n",
    "\n",
    "plt.plot(x_axis, data_train[dependent_variable], linewidth=6, color = dark_green) \n",
    "plt.plot(y_axis, data_test[dependent_variable], linewidth=6, color = pale_green) \n",
    "\n",
    "plt.ylim(bottom=0)\n",
    "plt.ylabel(dependent_variable + ' Training vs. Testing', fontsize=20)\n",
    "plt.xlabel(\"Records\", fontsize=20)\n",
    "\n",
    "\n",
    "plt.xticks(fontsize=16) \n",
    "plt.yticks(fontsize=16)\n",
    "plt.grid(visible = True, color =brown, axis ='both')\n",
    "\n",
    "plt.legend([\"Training\", \"Testing\"], loc='best', fontsize=20)\n",
    "\n",
    "fname = figures_path + dependent_variable[2:len(dependent_variable)] + \"_\" + \"Daily_Training_Testing_Subsets_Average_Site.png\"\n",
    "plt.savefig(fname, format='png')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Data Standardization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data is re-scaled standardizing the numerical columns by presenting a mean of 0 and a standard deviation of 1 thus all elements independently of their dimension can be compared.\n",
    "\n",
    "The pre-processing transformer StandScaler() from Scikit-learn package is directly applied in the Forecaster Class that will run the regression models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Daily Data Description\n",
    "\n",
    "### 2.1. Dependent variable plot\n",
    "\n",
    "We plot the dependent variable at daily granularity level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the dependent variable\n",
    "fig, ax = plt.subplots(figsize=(30,10))\n",
    "\n",
    "ax.plot(df_multivar.index,df_multivar[dependent_variable],color=dark_green,linewidth=6, label=dependent_variable)\n",
    "\n",
    "plt.title(dependent_variable + \" Daily Training & Testing Subsets (Average Site)\", fontsize = 30,  loc = 'left', fontweight=\"bold\")\n",
    "\n",
    "locs, labels = plt.xticks() \n",
    "newlocs  = np.arange(0, len(locs), 50)\n",
    "newlabels = []\n",
    "\n",
    "for i in range(0, len(newlocs)): \n",
    "    newlabels.append(df_multivar.index[newlocs[i]])\n",
    "    \n",
    "\n",
    "ax.set_ylim(bottom=0)\n",
    "plt.ylabel(dependent_variable, size=30)\n",
    "plt.xlabel(\"date\", size=30)\n",
    "plt.xticks(fontsize=16, ticks=newlocs, labels=newlabels, minor = False )\n",
    "plt.yticks(fontsize=16)\n",
    "plt.legend(loc=\"best\",fontsize=\"20\")\n",
    "plt.grid(visible = True, color =brown, axis ='both')\n",
    "\n",
    "fname = figures_path + dependent_variable[2:len(dependent_variable)] + \"_by_date.png\"\n",
    "plt.savefig(fname, format='png')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Autocorrelation plots\n",
    "\n",
    "Autocorrelation and partial autocorrelation function are plotted to show the dependent's variable autorregresive behavior that justify the usage of autorregresive/recursive forecasting methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Autocorrelation plot\n",
    "# ======================================================================================\n",
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(30, 10), sharex=True)\n",
    "\n",
    "sm.graphics.tsa.plot_acf(df_multivar[dependent_variable],ax=ax, title = '', lags=24)\n",
    "\n",
    "plt.title(dependent_variable + ' Daily Autocorrelation Function Plot', fontsize=30, loc = 'left', fontweight=\"bold\")\n",
    "\n",
    "plt.ylabel(dependent_variable + ' Autocorrelation', fontsize=20)\n",
    "plt.xlabel(\"Lag\", fontsize=20)\n",
    "\n",
    "\n",
    "plt.xticks(fontsize=16) \n",
    "plt.yticks(fontsize=16)\n",
    "plt.grid(visible = True, color =brown, axis ='both')\n",
    "\n",
    "plt.legend([\"Autocorrelation\"], loc='best', fontsize=20)\n",
    "\n",
    "fname = figures_path + dependent_variable[2:len(dependent_variable)] + '_Daily_Autocorrelation_Function_Plot.png'\n",
    "plt.savefig(fname, format='png')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Partial Autocorrelation plot\n",
    "# ======================================================================================\n",
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(30, 10), sharex=True)\n",
    "\n",
    "sm.graphics.tsa.plot_pacf(df_multivar[dependent_variable],ax=ax, title = '', lags=24, method='ywm')\n",
    "\n",
    "plt.title(dependent_variable + ' Daily Partial Autocorrelation Function Plot', fontsize=30, loc = 'left', fontweight=\"bold\")\n",
    "\n",
    "plt.ylabel(dependent_variable + ' Partial Autocorrelation', fontsize=20)\n",
    "plt.xlabel(\"Lag\", fontsize=20)\n",
    "\n",
    "\n",
    "plt.xticks(fontsize=16) \n",
    "plt.yticks(fontsize=16)\n",
    "plt.grid(visible = True, color =brown, axis ='both')\n",
    "\n",
    "plt.legend([\"Partial Autocorrelation\"], loc='best', fontsize=20)\n",
    "\n",
    "fname = figures_path + dependent_variable[2:len(dependent_variable)] + '_Daily_Partial_Autocorrelation_Function_Plot.png'\n",
    "plt.savefig(fname, format='png')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Recursive Forecasting Models withe the Sliding Window\n",
    "\n",
    "To predict variables at t$_n$ we need to have variables at t$_{n−1}$ whose values are unknown. A recursive process is applied in which, each new prediction, is based on the previous one. This process is called _recursive forecasting_.\n",
    "\n",
    "\n",
    "The model specification is the following:\n",
    "\n",
    "\n",
    "$\\qquad\\qquad$ Dependent Variable$_t = f($Dependent Variable$_{t−1} \\quad + \\quad $exogenous variables$_{t−1})$ + $\\epsilon_t$\n",
    "\n",
    "<br>\n",
    " \n",
    "<div>\n",
    "<img src=\"attachment:diagram-recursive-mutistep-forecasting.png\" width=\"500\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Forecaster creation and training\n",
    "\n",
    "With the ForecasterAutoreg class, a model is created and trained from a __SciKit Learn__ regressor with a time window of 24 lags (input window). This means that the model uses the previous 24 hours as predictors. We also add the lagged exogenous variables as inputs of our models. We train the selected regressor with default options to produce an outpput window of 24 leads (we will predict the following 24 hours):  \n",
    "\n",
    "<ol>\n",
    "    <li>Ordinary Least Squares Regression</li>\n",
    "    <li>Lasso (alpha = 0.1)</li>\n",
    "    <li>Ridge Regression (alpha = 0.5)</li>\n",
    "    <li>Bayesian Ridge Regression</li>\n",
    "    <li>Random Forest (n_estimators = 100, max_depth = 5)</li>  \n",
    "    <li>Gradient Boosting Regressor (n_estimators = 100, max_depth = 5, learning_rate = 0.1)</li>  \n",
    "</ol>\n",
    "\n",
    "#### 3.1.1 Ordinary Least Squares Regression\n",
    "\n",
    "According to scikit-learn \"_LinearRegression fits a linear model with coefficients  to minimize the residual sum of squares between the observed targets in the dataset, and the targets predicted by the linear approximation. Mathematically it solves a problem of the form:\"_\n",
    "\n",
    " \n",
    "<p style=\"text-align: center;\">$\\min_{w} || X w - y||_2^2$</p>\n",
    "\n",
    "#### 3.1.2 Lasso (L1 Norm)\n",
    "\n",
    "According to scikit-learn \"_The Lasso (Least Absolute Shrinkage and Selection Operator) is a linear model that estimates sparse coefficients. It is useful in some contexts due to its tendency to prefer solutions with fewer non-zero coefficients, effectively reducing the number of features upon which the given solution is dependent._\n",
    "\n",
    "_Mathematically, it consists of a linear model with an added regularization term. The objective function to minimize is:_\n",
    "\n",
    "<p style=\"text-align: center;\">$\\min_{w} { \\frac{1}{2n_{\\text{samples}}} ||X w - y||_2 ^ 2 + \\alpha ||w||_1}$</p>\n",
    " \n",
    "_The lasso estimate thus solves the minimization of the least-squares penalty with  added, where $\\alpha$ is a constant and $||w||_1$ is the norm of the coefficient vector._ \"\n",
    "\n",
    "#### 3.1.3 Ridge Regression (L2 Norm)\n",
    "\n",
    "According to scikit-learn \"_Ridge regression addresses some of the problems of Ordinary Least Squares by imposing a penalty on the size of the coefficients. The ridge coefficients minimize a penalized residual sum of squares:_\n",
    "\n",
    " \n",
    "<p style=\"text-align: center;\">$\\min_{w} || X w - y||_2^2 + \\alpha ||w||_2^2$</p>\n",
    "\n",
    "_The complexity parameter $\\alpha \\ge 0$ controls the amount of shrinkage: the larger the value of \n",
    "$\\alpha$, the greater the amount of shrinkage and thus the coefficients become more robust to collinearity._ \"\n",
    "\n",
    "<br>\n",
    "<div>\n",
    "<img src=\".\\lasso-ridge.png\" width=\"500\"/>\n",
    "</div>\n",
    "\n",
    "#### 3.1.4 Bayesian Ridge Regression\n",
    "\n",
    "According to scikit-learn \"_BayesianRidge estimates a probabilistic model of the regression problem as described above. The prior for the coefficient w is given by a spherical Gaussian:_  $p(w|\\lambda) = \\mathcal{N}(w|0,\\lambda^{-1}\\mathbf{I}_{p})$\n",
    "\n",
    "_The priors over $\\alpha$ and $\\lambda$ are chosen to be gamma distributions, the conjugate prior for the precision of the Gaussian. The resulting model is called Bayesian Ridge Regression, and is similar to the classical Ridge. The parameters $w,\\alpha$ and $\\lambda$ are estimated jointly during the fit of the model, the regularization parameters $\\alpha$ and $\\lambda$ being estimated by maximizing the log marginal likelihood._\n",
    "_There are four more hyperparameters, $\\alpha_1, \\alpha_2, \\lambda_1, and \\lambda_2$ of the gamma prior distributions over $\\alpha$ and $\\lambda$. These are usually chosen to be non-informative. By default $\\alpha_1 = \\alpha_2 =  \\lambda_1 = \\lambda_2 = 10^{-6}$_ \"\n",
    "\n",
    "<br>\n",
    "<div>\n",
    "<img src=\".\\bayesian.png\" width=\"700\"/>\n",
    "</div>\n",
    "\n",
    "\n",
    "#### 3.1.5 Random Forest\n",
    "\n",
    "According to scikit-learn \"_in random forests each tree in the ensemble is built from a sample drawn with replacement (i.e., a bootstrap sample) from the training set. Furthermore, when splitting each node during the construction of a tree, the best split is found either from all input features or a random subset of size max_features._\n",
    "\n",
    "_The purpose of these two sources of randomness is to decrease the variance of the forest estimator. Indeed, individual decision trees typically exhibit high variance and tend to overfit. The injected randomness in forests yield decision trees with somewhat decoupled prediction errors. By taking an average of those predictions, some errors can cancel out. Random forests achieve a reduced variance by combining diverse trees, sometimes at the cost of a slight increase in bias. In practice the variance reduction is often significant hence yielding an overall better model._\n",
    "\n",
    "_The scikit-learn implementation combines classifiers by averaging their probabilistic prediction, instead of letting each classifier vote for a single class._\n",
    "\n",
    "_The main parameters to adjust when using these methods is_ __n_estimators__ _(the former is the number of trees in the forest) and_ __max_features__ _(size of the random subsets of features to consider when splitting a node)._ \"\n",
    "\n",
    "<br>\n",
    "<div>\n",
    "<img src=\".\\rt.png\" width=\"400\"/>\n",
    "</div>\n",
    "\n",
    "#### 3.1.6 Gradient Boosting Regressor\n",
    "\n",
    "According to scikit-learn \"_Gradient Tree Boosting is a generalization of boosting to arbitrary differentiable loss functions. GBDT is an accurate and effective off-the-shelf procedure that can be used for regression problems. GradientBoostingRegressor is useful when the number of samples is larger than tens of thousands of samples. This method has built-in support for missing values, which avoids the need for an imputer._\n",
    "\n",
    "_The 2 most important parameters of these estimators are_ __n_estimators__  _(number of weak learners) and_ __learning_rate__ _(hyperparameter that controls overfitting via shrinkage)._ \"\n",
    "\n",
    "GBRT regressors are additive models whose prediction $\\hat{y}_i$ for a given input  is of the following form:\n",
    "\n",
    "<p style=\"text-align: center;\">$\\hat{y}_i = F_M(x_i) = \\sum_{m=1}^{M} h_m(x_i)$</p>\n",
    " \n",
    "where the $h_m$ are estimators called weak learners in the context of boosting. Gradient Tree Boosting uses decision tree regressors of fixed size as weak learners. The constant M corresponds to the __n_estimators__ parameter.\n",
    "\n",
    "Similar to other boosting algorithms, a GBRT is built in a greedy fashion:\n",
    "\n",
    "<p style=\"text-align: center;\">$F_m(x) = F_{m-1}(x) + h_m(x)$</p>\n",
    "\n",
    "where the newly added tree $h_m$ is fitted in order to minimize a sum of losses $L_m$, given the previous ensemble $F_{m-1}$:\n",
    "\n",
    "<p style=\"text-align: center;\">$h_m =  \\arg\\min_{h} L_m = \\arg\\min_{h} \\sum_{i=1}^{n} l(y_i, F_{m-1}(x_i) + h(x_i))$</p>\n",
    "\n",
    "\n",
    " \n",
    "where $l(y_i, F(x_i))$ is defined by the __loss__ parameter.\n",
    "\n",
    "By default, the initial model  is chosen as the constant that minimizes the loss: for a least-squares loss, this is the empirical mean of the target values. \n",
    "\n",
    "Using a first-order Taylor approximation, the value of  can be approximated as follows:\n",
    "\n",
    "<p style=\"text-align: center;\">$\n",
    "l(y_i, F_{m-1}(x_i) + h_m(x_i)) \\approx\n",
    "l(y_i, F_{m-1}(x_i))\n",
    "+ h_m(x_i)\n",
    "\\left[ \\frac{\\partial l(y_i, F(x_i))}{\\partial F(x_i)} \\right]_{F=F_{m - 1}}$</p>\n",
    "\n",
    "The quantity $\\left[ \\frac{\\partial l(y_i, F(x_i))}{\\partial F(x_i)} \\right]_{F=F_{m - 1}}$ is the derivative of the loss with respect to its second parameter, evaluated at . It is easy to compute for any given  in a closed form since the loss is differentiable. It is denoted by $g_i$\n",
    "\n",
    "Removing the constant terms, we have:\n",
    "\n",
    "<p style=\"text-align: center;\">$h_m \\approx \\arg\\min_{h} \\sum_{i=1}^{n} h(x_i) g_i$</p>\n",
    "\n",
    "This is minimized if  is fitted to predict a value that is proportional to the negative gradient . Therefore, at each iteration, the estimator  is fitted to predict the negative gradients of the samples. The gradients are updated at each iteration. This can be considered as some kind of gradient descent in a functional space.\n",
    "\n",
    "<br>\n",
    "<div>\n",
    "<img src=\".\\xgb.png\" width=\"500\"/>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor_named"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Options for other regressors included in Scikit-Learn: \n",
    "# 1-LinearRegression, 2-Lasso, 3-RidgeRegression, 4-ElasticNet, 5-Random Forest, 6-XGBRegressor\n",
    "\n",
    "if regressor_named == \"Linear Regression\":\n",
    "    selected_regressor = LinearRegression()    \n",
    "    \n",
    "elif regressor_named ==  \"Lasso\":\n",
    "    selected_regressor = Lasso(alpha=alpha)\n",
    "    \n",
    "elif regressor_named == \"Ridge Regression\":\n",
    "    selected_regressor = Ridge(alpha=alpha)\n",
    "    \n",
    "elif regressor_named ==  \"Bayesian Ridge Regression\":\n",
    "    selected_regressor = BayesianRidge()\n",
    "    \n",
    "elif regressor_named == \"Random Forest Regressor\":\n",
    "    \n",
    "    selected_regressor = RandomForestRegressor(random_state=123, max_features=10)\n",
    "    \n",
    "else:\n",
    "    \n",
    "    selected_regressor = GradientBoostingRegressor(random_state=123, max_features=10)\n",
    "\n",
    "print(selected_regressor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lags = 7   # lags horizon as input window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input X contains Nans\n",
    "data_train.fillna(axis = 1, value= 0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove index\n",
    "\n",
    "#data_train = data_train.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train RF forecaster \n",
    "# We utilize the pre-processing StandardScaler() from Scikit-learn package directly in the SKForecast package\n",
    "# ===========================================================================================================\n",
    "\n",
    "if regressor_named == \"Linear Regression\" or regressor_named ==  \"Lasso\" or regressor_named == \"Ridge Regression\" or regressor_named ==  \"Bayesian Ridge Regression\":\n",
    "    \n",
    "    forecaster = ForecasterAutoreg(\n",
    "                regressor = selected_regressor,\n",
    "                lags      = lags,\n",
    "                transformer_y = StandardScaler(),\n",
    "                transformer_exog = StandardScaler()\n",
    "             )\n",
    "\n",
    "    forecaster.fit(y=data_train[dependent_variable])\n",
    "        \n",
    "elif regressor_named == \"Random Forest Regressor\":\n",
    "    \n",
    "    forecaster = ForecasterAutoreg(\n",
    "                regressor = selected_regressor,\n",
    "                lags      = lags,\n",
    "                transformer_y = StandardScaler(),\n",
    "                transformer_exog = StandardScaler()\n",
    "             )\n",
    "\n",
    "    forecaster.fit(y=data_train[dependent_variable], exog=data_train[exog_variables])\n",
    "       \n",
    "elif regressor_named == \"XGBoost\":\n",
    "    \n",
    "    forecaster = ForecasterAutoreg(\n",
    "                 regressor = selected_regressor, #GradientBoostingRegressor(random_state=123, max_features=10),\n",
    "                 lags = lags,\n",
    "                 transformer_y = StandardScaler(),\n",
    "                 transformer_exog = StandardScaler()\n",
    "             )\n",
    "    forecaster.fit(y=data_train[dependent_variable], exog=data_train[exog_variables])\n",
    "\n",
    "forecaster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of exogenous variables\n",
    "len(exog_variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making predictions\n",
    "# ==============================================================================\n",
    "# predict(self, steps, last_window=None, exog=None)\n",
    "# Predict n steps ahead. It is an recursive process in which, each prediction, is used as a predictor \n",
    "# for the next step. \n",
    "\n",
    "if regressor_named == \"Linear Regression\" or regressor_named ==  \"Lasso\" or regressor_named == \"Ridge Regression\" or regressor_named ==  \"Bayesian Ridge Regression\":\n",
    "\n",
    "    predictions = forecaster.predict_interval(steps=steps, interval = [5, 95], random_state = 123, n_boot=500)\n",
    "        \n",
    "elif regressor_named == \"Random Forest Regressor\" or regressor_named == \"XGBoost\":\n",
    "    \n",
    "    predictions = forecaster.predict(steps=steps, exog=data_test[exog_variables])\n",
    "       \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = figures_path + dependent_variable[2:len(dependent_variable)] + '_' + regressor_named + '_Daily_Forecasting_Testing_Subsets_Average_Site.png'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = figures_path + dependent_variable[2:len(dependent_variable)] + '_' + regressor_named + '_Daily_Forecasting_Testing_Subsets_Average_Site.png'\n",
    "# Plot Forecasting vs Testing subsets\n",
    "\n",
    "if regressor_named == \"Linear Regression\" or regressor_named ==  \"Lasso\" or regressor_named == \"Ridge Regression\" or regressor_named ==  \"Bayesian Ridge Regression\":\n",
    "    \n",
    "    plt.figure(figsize=(30, 10))\n",
    "\n",
    "    plt.title(dependent_variable + ' ' + regressor_named + ' Daily Forecasting & Testing Subsets (Average Site)', fontsize=30, loc = 'left', fontweight=\"bold\")\n",
    "\n",
    "    x_axis = data_test[dependent_variable][:steps].index\n",
    "    plt.plot(x_axis, data_test[dependent_variable][:steps], linewidth=6, color = dark_blue ) \n",
    "    plt.plot(x_axis, predictions.pred, linewidth=6, color = pale_red) \n",
    "    plt.fill_between(x_axis, predictions['lower_bound'],predictions['upper_bound'],color = pale_green,alpha = 0.2)\n",
    "\n",
    "    plt.ylabel(dependent_variable + ' Forecasting vs. Testing', fontsize=20)\n",
    "    plt.xlabel(\"Date\", fontsize=20)\n",
    "\n",
    "\n",
    "    plt.xticks(fontsize=16) \n",
    "    plt.yticks(fontsize=16)\n",
    "    plt.grid(visible = True, color =brown, axis ='both')\n",
    "\n",
    "    plt.legend([\"Testing\", \"Forecasting\"], loc='best', fontsize=20)\n",
    "\n",
    "    plt.savefig(fname, format='png')\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "            \n",
    "elif regressor_named == \"Random Forest Regressor\" or regressor_named == \"XGBoost\":\n",
    "    \n",
    "    plt.figure(figsize=(30, 10))\n",
    "\n",
    "    x_test_axis = data_test.index[:steps]\n",
    "    x_fcst_axis = predictions.index[:steps]\n",
    "\n",
    "    plt.title(dependent_variable + ' ' + regressor_named + ' Daily Forecasting & Testing Subsets (Average Site)', fontsize=30, loc = 'left', fontweight=\"bold\")\n",
    "\n",
    "    plt.plot(x_test_axis, data_test[dependent_variable][:steps], linewidth=6, color = pale_green)\n",
    "    plt.plot(x_fcst_axis, predictions, 'ko')\n",
    "\n",
    "    plt.ylabel(dependent_variable + ' ' + regressor_named + ' Forecasting vs. Testing', fontsize=20)\n",
    "    plt.xlabel(\"Date\", fontsize=20)\n",
    "\n",
    "    plt.xticks(fontsize=16)\n",
    "    plt.yticks(fontsize=16)\n",
    "    plt.grid(visible = True, color = brown, axis ='both')\n",
    "\n",
    "    plt.legend([\"Testing\", \"Forecasting\"], loc='best', fontsize=20)\n",
    "\n",
    "    plt.savefig(fname, format='png')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor_named"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Forecast Error Regression Metrics\n",
    "# ==============================================================================\n",
    "    \n",
    "if regressor_named == \"Linear Regression\" or regressor_named ==  \"Lasso\" or regressor_named == \"Ridge Regression\" or regressor_named ==  \"Bayesian Ridge Regression\": \n",
    "\n",
    "    # RMSE\n",
    "    print(\"Root Mean squared error: {:.2f}\".format(np.sqrt(mean_squared_error(data_test[dependent_variable][:steps], predictions.pred))))\n",
    "    # MSE\n",
    "    print(\"Mean squared error: {:.2f}\".format(mean_squared_error(data_test[dependent_variable][:steps], predictions.pred)))\n",
    "    # MAE\n",
    "    print(\"Mean absolute error: {:.2f}\".format(mean_absolute_error(data_test[dependent_variable][:steps], predictions.pred)))\n",
    "    # R2\n",
    "    print(\"Coefficient of determination:{:.2%}\".format(r2_score(data_test[dependent_variable][:steps], predictions.pred)))\n",
    "        \n",
    "elif regressor_named == \"Random Forest Regressor\" or regressor_named == \"XGBoost\":\n",
    "\n",
    "    # RMSE\n",
    "    print(\"Root Mean squared error: {:.2f}\".format(np.sqrt(mean_squared_error(data_test[dependent_variable][:steps], predictions.values))))\n",
    "    # MSE\n",
    "    print(\"Mean squared error: {:.2f}\".format(mean_squared_error(data_test[dependent_variable][:steps], predictions.values)))\n",
    "    # MAE\n",
    "    print(\"Mean absolute error: {:.2f}\".format(mean_absolute_error(data_test[dependent_variable][:steps], predictions.values)))\n",
    "    # R2\n",
    "    print(\"Coefficient of determination:{:.2%}\".format(r2_score(data_test[dependent_variable][:steps], predictions.values)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Hyperparameter Tuning (non-linear regressors)\n",
    "\n",
    "_Grid_search_forecaster_ method is used with non-linear regressors (Random Forest and eXtreme Gradient Boost) in order to optimize the performance by tuning the hyperparameter selection (number of trees, max tree depth, learning rate, etc.) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Hyperparameter Grid search ---> only for not linear\n",
    "# ==============================================================================\n",
    "\n",
    "if regressor_named == \"Random Forest Regressor\" or regressor_named == \"XGBoost\":\n",
    "    \n",
    "    forecaster = ForecasterAutoreg(\n",
    "                regressor = selected_regressor,\n",
    "                lags      = lags\n",
    "             )\n",
    "\n",
    "    # Lags used as predictors\n",
    "    lags_grid = [12, 24]\n",
    "\n",
    "    # Random Forest's hyperparameters\n",
    "    \n",
    "        # n_estimators: The number of trees in the forest.\n",
    "        # max_depth: the maximum depth of the tree. If None, then nodes are expanded until all leaves are \n",
    "                    # pure or until all leaves contain less than min_samples_split samples.\n",
    "        \n",
    "    # Gradient Boosting Regressor's hyperparameters\n",
    "    \n",
    "        # n_estimators: The number of boosting stages to perform\n",
    "        # max_depth: Maximum depth of the individual regression estimators.\n",
    "        # learning_rate: Learning rate shrinks the contribution of each tree by learning_rate. \n",
    "                        # There is a trade-off between learning_rate and n_estimators\n",
    "    \n",
    "    param_grid = {'n_estimators': [100], 'max_depth': [5, 10]}\n",
    "    \n",
    "    results_grid = grid_search_forecaster(\n",
    "                    forecaster  = forecaster,\n",
    "                    y           = data_train[dependent_variable],\n",
    "                    exog        = data_train[exog_variables],\n",
    "                    param_grid  = param_grid,\n",
    "                    lags_grid   = lags_grid,\n",
    "                    steps       = steps,\n",
    "                    refit       = False,\n",
    "                    metric      = 'mean_squared_error',\n",
    "                    initial_train_size = int(len(data_train)*0.5),\n",
    "                    fixed_train_size   = False,\n",
    "                    return_best = True,\n",
    "                    verbose     = False\n",
    "                                    )\n",
    "     # Grid Search results\n",
    "    # ==============================================================================\n",
    "    results_grid\n",
    "\n",
    "    # Best hyperparameters\n",
    "\n",
    "    lags_best = int(results_grid.lags[results_grid.index[0]].max())\n",
    "    n_estimators_best = results_grid.n_estimators[results_grid.index[0]]\n",
    "    max_depth_best = results_grid.max_depth[results_grid.index[0]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Final Model\n",
    "\n",
    "Finally, the ForecasterAutoreg model is trained and a validated setting the optimal configuration\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create, train, and make predictions with the best forecaster\n",
    "# ============================================================\n",
    "    \n",
    "if regressor_named == \"Random Forest Regressor\" or regressor_named == \"XGBoost\":\n",
    "    \n",
    "    if regressor_named == \"Random Forest Regressor\":\n",
    "        \n",
    "        selected_regressor = RandomForestRegressor(max_depth = max_depth_best, \n",
    "                                  n_estimators = n_estimators_best, \n",
    "                                  random_state = 123)\n",
    "        \n",
    "        best_forecaster = ForecasterAutoreg(\n",
    "                regressor = selected_regressor,\n",
    "                lags      = lags_best, # lags demands for an integer that is why we used int()\n",
    "                transformer_y = StandardScaler(),\n",
    "                transformer_exog = StandardScaler()\n",
    "             )\n",
    "                \n",
    "        best_forecaster.fit(y=data_test[dependent_variable], exog=data_test[exog_variables]) # training\n",
    "        predictions = best_forecaster.predict(steps=steps, exog=data_predict[exog_variables])\n",
    "        \n",
    "    elif regressor_named == \"XGBoost\":\n",
    "     \n",
    "        selected_regressor = GradientBoostingRegressor(max_depth = max_depth_best, \n",
    "                                  n_estimators = n_estimators_best, learning_rate=0.1,\n",
    "                                  random_state = 123)\n",
    "     \n",
    "        best_forecaster = ForecasterAutoreg(\n",
    "                regressor = selected_regressor,\n",
    "                lags      = lags_best, # lags demands for an integer that is why we used int()\n",
    "                transformer_y = StandardScaler(),\n",
    "                transformer_exog = StandardScaler()\n",
    "             )\n",
    "\n",
    "        best_forecaster.fit(y=data_test[dependent_variable], exog=data_test[exog_variables]) # training\n",
    "        predictions = best_forecaster.predict(steps=steps, exog=data_predict[exog_variables])\n",
    "    \n",
    "    else:\n",
    "    \n",
    "        best_forecaster = forecaster\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Forecasting with the Best Forecaster (Average Site)\n",
    "# =============================================================================\n",
    "\n",
    "if regressor_named == \"Random Forest Regressor\" or regressor_named == \"XGBoost\":\n",
    "\n",
    "    plt.figure(figsize=(30, 10))\n",
    "\n",
    "    x_test_axis = data_predict.index\n",
    "    x_fcst_axis = predictions.index\n",
    " \n",
    "    plt.title(dependent_variable + ' ' + regressor_named + ' Daily Forecasting & Testing Subsets (Average Site)', fontsize=30, loc = 'left', fontweight=\"bold\")\n",
    "    plt.plot(x_test_axis, data_predict[dependent_variable], linewidth=7.0, color = pale_green)\n",
    "    plt.plot(x_fcst_axis, predictions, 'ko')\n",
    "\n",
    "    plt.xticks(fontsize=16) \n",
    "    plt.yticks(fontsize=16)\n",
    "    plt.grid(visible = True, color = brown, axis ='both')\n",
    "\n",
    "    plt.legend([\"Testing\", \"Forecasting\"], loc='best', fontsize=20)\n",
    "\n",
    "    fname = figures_path + dependent_variable[2:len(dependent_variable)] + '_' + regressor_named + '_Daily_Forecasting_Testing_Subsets_Average_Site.png'\n",
    "    plt.savefig(fname, format='png')\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forecast Error Regression Metrics\n",
    "# ==============================================================================\n",
    "\n",
    "if regressor_named == \"Random Forest Regressor\" or regressor_named == \"XGBoost\":\n",
    "\n",
    "    # RMSE\n",
    "    print(\"Root Mean squared error: {:.2f}\".format(np.sqrt(mean_squared_error(data_predict[dependent_variable][:steps], predictions))))\n",
    "    #MSE\n",
    "    print(\"Mean squared error: {:.2f}\".format(mean_squared_error(data_predict[dependent_variable][:steps], predictions)))\n",
    "    # MAE\n",
    "    print(\"Mean absolute error: {:.2f}\".format(mean_absolute_error(data_predict[dependent_variable][:steps], predictions)))\n",
    "    # R2\n",
    "    print(\"Coefficient of determination:{:.2%}\".format(r2_score(data_predict[dependent_variable][:steps], predictions)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4. Backtesting\n",
    "\n",
    "Backtesting is way of testing if a model’s predictions are in line with realised data. Backtesting a production model, for instance, is typically done by checking if actual historical production during a particular period is consistently higher, the model is underestimating production. If they are lower, the model is overestimating production.\n",
    "\n",
    "##### Backtesting with refit and increasing training size (fixed origin)\n",
    "\n",
    "The model is trained each time before making predictions. With this configuration, the model uses all the data available so far. It is a variation of the standard cross-validation but, instead of making a random distribution of the observations, the training set increases sequentially, maintaining the temporal order of the data.\n",
    "\n",
    "<br>\n",
    "<div>\n",
    "<img src=\"attachment:diagram-backtesting-refit.png\" width=\"600\"/>\n",
    "</div>\n",
    "\n",
    "In the Figure we see a time series backtesting diagram with an initial training size of 10 observations, a prediction horizon of 3 steps, and retraining at each iteration. Instead of randomizing the data, this backtesting sequentially increases the size of the training set while maintaining the temporal order of the data. By doing this, the model can be tested on progressively larger amounts of historical data, providing a more accurate assessment of its predictive capabilities.\n",
    "\n",
    "<br>\n",
    "<div>\n",
    "<img src=\"attachment:backtesting_refit.gif\" width=\"700\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backtesting with refit and increasing training size (fixed origin)\n",
    "# ==============================================================================\n",
    "\n",
    "n_backtesting = steps * 3 # Backtesting is three times the forecasting horizon\n",
    "\n",
    "if regressor_named == \"Linear Regression\" or regressor_named ==  \"Lasso\" or regressor_named == \"Ridge Regression\" or regressor_named ==  \"Bayesian Ridge Regression\":\n",
    "\n",
    "    metric, predicciones = backtesting_forecaster(\n",
    "                            forecaster         = forecaster,\n",
    "                            y                  = data_test[dependent_variable],\n",
    "                            initial_train_size = len(data_test) - n_backtesting,\n",
    "                            fixed_train_size   = False,\n",
    "                            steps              = steps,\n",
    "                            metric             = 'mean_squared_error',\n",
    "                            refit              = True,\n",
    "                            interval           = [1, 99],\n",
    "                            n_boot             = 100,\n",
    "                            verbose            = True\n",
    "                            )\n",
    "    print(f\"Backtest error: {metric}\")\n",
    "    \n",
    "elif regressor_named == \"Random Forest Regressor\" or regressor_named == \"XGBoost\":\n",
    "\n",
    "    \n",
    "\n",
    "    metric, predictions_backtest = backtesting_forecaster(\n",
    "                                    forecaster         = best_forecaster,\n",
    "                                    y                  = data_test[dependent_variable],\n",
    "                                    exog               = data_test[exog_variables],\n",
    "                                    initial_train_size = len(data_test) - n_backtesting,\n",
    "                                    fixed_train_size   = False,\n",
    "                                    steps              = steps,\n",
    "                                    metric             = 'mean_squared_error',\n",
    "                                    refit              = True,\n",
    "                                    verbose            = True\n",
    "                                )\n",
    "    \n",
    "    print(f\"Backtest error: {metric}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if regressor_named == \"Random Forest Regressor\" or regressor_named == \"XGBoost\":\n",
    "\n",
    "    # Get predictions from backtesting \n",
    "    print(predictions_backtest.tail(24))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.  Saving models\n",
    "\n",
    "Skforecast models are stored by using save_forecaster method. Later they can be loaded with load_forecaster method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save selected regressor model with dependent and exogenous variables\n",
    "\n",
    "if regressor_named == \"Linear Regression\" or regressor_named ==  \"Lasso\" or regressor_named == \"Ridge Regression\" or regressor_named ==  \"Bayesian Ridge Regression\":\n",
    "\n",
    "    save_forecaster(forecaster, file_name = models_path + dependent_variable + '_' +regressor_named + '_forecaster.py', verbose=False) \n",
    "            \n",
    "elif regressor_named == \"Random Forest Regressor\" or regressor_named == \"XGBoost\":\n",
    "\n",
    "    save_forecaster(best_forecaster, file_name = models_path + dependent_variable + '_' +regressor_named + '_forecaster.py', verbose=False) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computation time\n",
    "\n",
    "time_end = time.localtime()\n",
    "\n",
    "delta =  (time.mktime(time_end) - time.mktime(time_init)) / 60\n",
    "\n",
    "print(\"Computation time = {:.2f}\".format(delta), \"minutes\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
