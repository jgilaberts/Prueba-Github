{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-15T21:29:00.770627Z",
     "start_time": "2019-05-15T21:28:59.068546Z"
    }
   },
   "source": [
    "# HW 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Miembros del grupo:\n",
    "- \n",
    "- \n",
    "-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-15T21:29:01.825376Z",
     "start_time": "2019-05-15T21:29:00.778620Z"
    }
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_parquet('../data/prosperLoanData_processed_train.parquet')\n",
    "df_val = pd.read_parquet('../data/prosperLoanData_processed_val.parquet')\n",
    "df_oot = pd.read_parquet('../data/prosperLoanData_processed_oot.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-15T21:29:01.860313Z",
     "start_time": "2019-05-15T21:29:01.832970Z"
    }
   },
   "outputs": [],
   "source": [
    "#drop indeterminates\n",
    "df_train = df_train.loc[df_train.indeterm==False]\n",
    "df_val = df_val.loc[df_val.indeterm==False]\n",
    "df_oot = df_oot.loc[df_oot.indeterm==False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-15T21:29:02.411374Z",
     "start_time": "2019-05-15T21:29:02.361592Z"
    }
   },
   "outputs": [],
   "source": [
    "#get the serialized data from previous session\n",
    "import pickle\n",
    "input_d2  = pickle.load(open('../prac5/output_HW5.pkl', 'rb'))\n",
    "corr_data, features = input_d2['corr_data'], input_d2['features']\n",
    "df_iv, df_psi = input_d2['iv_df'], input_d2['psi_df']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter out features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop highly correlated features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-15T21:29:04.428722Z",
     "start_time": "2019-05-15T21:29:04.326572Z"
    }
   },
   "outputs": [],
   "source": [
    "# corr_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-15T21:29:05.932319Z",
     "start_time": "2019-05-15T21:29:05.864461Z"
    }
   },
   "outputs": [],
   "source": [
    "#run through the iv ranking, and drop features if they are correlated with any feature with better ranking\n",
    "feats_sorted = df_iv.feature.values.tolist()\n",
    "\n",
    "def get_uncorr_feats(corr_data, feats_sorted):\n",
    "    \"\"\"Handles the corr_data structure, to drop highlly correlated features\n",
    "    \n",
    "    Args:\n",
    "        corr_data: List of tuples containing the correlation info\n",
    "        feats_sorted: List, with the features to be sorted / dropped\n",
    "        \n",
    "    Returns:\n",
    "        List with the features that have no correlation\n",
    "    \"\"\"\n",
    "    features_keep = feats_sorted[:1]\n",
    "    for feat in feats_sorted[1:]:\n",
    "        #capture the correlation tuple\n",
    "        crr_data = [crr for crr in corr_data if crr[0] == feat]\n",
    "        if len(crr_data):\n",
    "            #if there is a 'hit' with a feature in features_keep, do not include it\n",
    "            hit = len(set(crr_data[0][2].index.tolist()) & set(features_keep)) > 0\n",
    "            if hit:\n",
    "                print ('Drop: ' + feat)\n",
    "            else:\n",
    "                features_keep.append(feat)    \n",
    "        else:\n",
    "            features_keep.append(feat)\n",
    "    return features_keep\n",
    "\n",
    "features_keep = get_uncorr_feats(corr_data, feats_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-15T21:29:08.949388Z",
     "start_time": "2019-05-15T21:29:08.926962Z"
    }
   },
   "outputs": [],
   "source": [
    "len(features), len(features_keep), len(corr_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop features with low IV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-15T21:29:25.926972Z",
     "start_time": "2019-05-15T21:29:25.832601Z"
    }
   },
   "outputs": [],
   "source": [
    "# IV filtering with this treshold\n",
    "df_iv.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO 1** Implementa una función que devuelva solo las columnas que tengan un IV inferior a un umbral dado. Usa el dataframe df_iv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lowIVFeatures(df_iv, th=0.02):\n",
    "    \"\"\"Returns a list with the features that have an IV lower that th\n",
    "    \n",
    "    Args:\n",
    "        df_iv: Pandas Dataframe with the feature name and their corresponding IV\n",
    "        th: Threshold to be considered when filtering the features\n",
    "        \n",
    "    Returns:\n",
    "        List with features that have an IV lower than the threshold\n",
    "    \"\"\"\n",
    "    # IMPLEMENT YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-15T21:29:40.198467Z",
     "start_time": "2019-05-15T21:29:40.168231Z"
    }
   },
   "outputs": [],
   "source": [
    "# capture low IV features\n",
    "low_iv_feats = lowIVFeatures(df_iv)\n",
    "features_keep_iv = list(set(features_keep) - set(low_iv_feats))\n",
    "len(features), len(features_keep), len(features_keep_iv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop unstable features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-15T21:30:20.993098Z",
     "start_time": "2019-05-15T21:30:20.903295Z"
    }
   },
   "outputs": [],
   "source": [
    "# PSI filtering with this treshold\n",
    "df_psi.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO 2** Implementa una función que devuelva solo las columnas que tengan un PSI mayor a un umbral dado. Usa el dataframe df_psi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def highPSIFeatures(df_psi, TH_PSI=0.25):\n",
    "    \"\"\"Returns a list with the features that have a PSI higher that th\n",
    "    \n",
    "    Args:\n",
    "        df_iv: Pandas Dataframe with the feature name and their corresponding IV\n",
    "        th: Threshold to be considered when filtering the features\n",
    "        \n",
    "    Returns:\n",
    "        List with features that have an IV lower than the threshold\n",
    "    \"\"\"\n",
    "    # IMPLEMENT YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-15T21:30:21.189747Z",
     "start_time": "2019-05-15T21:30:21.156916Z"
    }
   },
   "outputs": [],
   "source": [
    "# capture high (unstable) features\n",
    "high_psi_features = highPSIFeatures(df_psi)\n",
    "features_keep_psi = list(set(features_keep_iv) - set(high_psi_features))\n",
    "len(features), len(features_keep), len(features_keep_iv), len(features_keep_psi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-15T21:30:23.068466Z",
     "start_time": "2019-05-15T21:30:23.042542Z"
    }
   },
   "outputs": [],
   "source": [
    "final_features = features_keep_psi\n",
    "final_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing - Set up bucketing - \"Trameado\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For numerical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-15T21:30:29.805950Z",
     "start_time": "2019-05-15T21:30:29.698613Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_bucket_numfeature(df, feat_col, n_bins, input_slider=(0., 100.)):\n",
    "    \"\"\"Cuts a numeric feature in 'n_bins', balacing data in percentiles\n",
    "    \n",
    "    Args:\n",
    "        df: Pandas DataFrame with the input data\n",
    "        feat_col: Name of the column with the input feature\n",
    "        obj_col: Name of the column with the target \n",
    "        n_bins: Number of cuts expected\n",
    "        input_slider: Range considered for the bucketing\n",
    "        \n",
    "    Returns:\n",
    "        List with the cuts corresponding to this feature\n",
    "    \"\"\"\n",
    "    #get the numeric input from the dual slider\n",
    "    # dividimos entre 100\n",
    "    perc_sliders = [v/100. for v in input_slider]\n",
    "    # calculamos los límites de la variable\n",
    "    var_lims = df[feat_col].quantile([perc_sliders[0], perc_sliders[1]]).values\n",
    "    v_min, v_max = var_lims[0], var_lims[1]\n",
    "    #filter the dataset using the slider input\n",
    "    df_cut = df.loc[(df[feat_col] <= v_max) & (df[feat_col] >= v_min)][[feat_col]]\n",
    "    cuts = df_cut[feat_col].quantile(np.linspace(perc_sliders[0], perc_sliders[1], n_bins + 1)).values.tolist()\n",
    "    cuts = sorted(list(set(cuts)))\n",
    "    return cuts\n",
    "\n",
    "def format_dummy_col(feat_col, dummy_col):\n",
    "    \"\"\"Handles column names for dummy data\n",
    "    \n",
    "    Args:\n",
    "        feat_col: Name of the column with the input feature\n",
    "        dummy_col: String of the dummy column \n",
    "        \n",
    "    Returns:\n",
    "        Dummy column with better formatting\n",
    "    \"\"\"\n",
    "    out = dummy_col.replace(\"(\", \"\")\\\n",
    "                   .replace(\"]\", \"\")\\\n",
    "                   .replace(\".0\", \"\")\\\n",
    "                   .replace(\", \", \"|\")\n",
    "    \n",
    "    return feat_col + '_' + out\n",
    "    \n",
    "def apply_bucketing_num(df, feat_col, cuts):\n",
    "    \"\"\"Applies bucketing to numerical feature\n",
    "    \n",
    "    Args:\n",
    "        df: Pandas Dataframe with the input data\n",
    "        feat_col: Name of the column with the input feature\n",
    "        cuts: Cuts that will be applied to the input data\n",
    "        \n",
    "    Returns:\n",
    "        Pandas dataframe with dummy columns\n",
    "    \"\"\"\n",
    "    cut_col = '{}_cut'.format(feat_col)\n",
    "    if len(cuts) == 2:\n",
    "        cuts = [cuts[0], np.mean(cuts), cuts[1]]\n",
    "    df[cut_col] = pd.cut(df[feat_col], cuts, include_lowest=True, precision=0)\n",
    "    if df[cut_col].isna().any():\n",
    "        df[cut_col] = df[cut_col].cat.add_categories([\"NA\"])\n",
    "        df[cut_col] = df[cut_col].fillna(\"NA\")\n",
    "        \n",
    "    dummies_df = pd.get_dummies(df[cut_col], drop_first=True)\n",
    "    dummies_df.columns = [format_dummy_col(feat_col, str(col)) for col in dummies_df.columns.values.tolist()]\n",
    "    \n",
    "    return dummies_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-15T21:30:31.225015Z",
     "start_time": "2019-05-15T21:30:31.182046Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_bucket_catfeature(df, feat_col, n_bins):\n",
    "    \"\"\"Cuts a categorical feature in 'n_bins', keeping categories with highest volume\n",
    "    \n",
    "    Args:\n",
    "        df: Pandas DataFrame with the input data\n",
    "        feat_col: Name of the column with the input feature\n",
    "        n_bins: Number of cuts expected\n",
    "        \n",
    "    Returns:\n",
    "        List with the cuts corresponding to this feature\n",
    "    \"\"\"\n",
    "    cuts = df.groupby(feat_col)[feat_col].count().sort_values(ascending=False)[:int(n_bins)].index.values.tolist()\n",
    "    \n",
    "    return cuts\n",
    "\n",
    "def apply_bucketing_cat(df, feat_col, cuts):\n",
    "    \"\"\"Applies bucketing to categorical feature\n",
    "    \n",
    "    Args:\n",
    "        df: Pandas Dataframe with the input data\n",
    "        feat_col: Name of the column with the input feature\n",
    "        cuts: Cuts that will be applied to the input data\n",
    "        \n",
    "    Returns:\n",
    "        Pandas dataframe with dummy columns\n",
    "    \"\"\"\n",
    "    cut_col = '{}_cut'.format(feat_col)\n",
    "    df[cut_col] = df[feat_col]\n",
    "    df.loc[~df[cut_col].isin(cuts), cut_col] = 'Other'\n",
    "    if df[cut_col].isna().any():\n",
    "        df[cut_col] = df[cut_col].fillna(\"NA\")\n",
    "        \n",
    "    dummies_df = pd.get_dummies(df[cut_col], prefix=feat_col, drop_first=True)\n",
    "    \n",
    "    return dummies_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-15T21:30:32.192798Z",
     "start_time": "2019-05-15T21:30:32.056734Z"
    }
   },
   "outputs": [],
   "source": [
    "#example of numerical bucketing\n",
    "cuts_num = get_bucket_numfeature(df_train, 'CreditScoreRangeLower', 6)\n",
    "dumm_num = apply_bucketing_num(df_train, 'CreditScoreRangeLower', cuts_num)\n",
    "dumm_num[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-15T21:30:33.717526Z",
     "start_time": "2019-05-15T21:30:33.599902Z"
    }
   },
   "outputs": [],
   "source": [
    "#example of categorical bucketing\n",
    "cuts_cat = get_bucket_catfeature(df_train, 'Occupation', 6)\n",
    "dumm_cat = apply_bucketing_cat(df_train, 'Occupation', cuts_cat)\n",
    "dumm_cat[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up bucketing, then apply it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-15T21:30:48.438303Z",
     "start_time": "2019-05-15T21:30:48.393550Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_bucket_feature(df, feat_col, n_bins=6):\n",
    "    \"\"\"Trains bucketing in a feature, whether if it is numerical\n",
    "    or categorical\n",
    "    \n",
    "    Args:\n",
    "        df: Pandas Dataframe with the input data\n",
    "        feat_col: Name of the column with the input feature\n",
    "        n_bins: Cuts that will be applied to the input data\n",
    "        \n",
    "    Returns:\n",
    "        List with the cuts learned from the data\n",
    "    \"\"\"\n",
    "    if df[feat_col].dtypes in (object, bool):\n",
    "        cuts = get_bucket_catfeature(df, feat_col, n_bins)\n",
    "    else:\n",
    "        cuts = get_bucket_numfeature(df, feat_col, n_bins)\n",
    "    return cuts\n",
    "\n",
    "def get_bucketing_allfeatures(df, features, n_bins=6):\n",
    "    \"\"\"Trains bucketing in all given features of a dataset\n",
    "    \n",
    "    Args:\n",
    "        df: Pandas Dataframe with the input data\n",
    "        features: Features which bucketing will be learnt\n",
    "        n_bins: Cuts that will be applied to the input data\n",
    "        \n",
    "    Returns:\n",
    "        Dict, containing all features and its corresponding\n",
    "        bucketing. For example:\n",
    "         {'feature1': cuts1,\n",
    "          'feature2': cuts2}  \n",
    "    \"\"\"\n",
    "    out_dict = {}\n",
    "    for feature in features:\n",
    "        print(feature)\n",
    "        cuts = get_bucket_feature(df, feature, n_bins)\n",
    "        out_dict[feature] = cuts\n",
    "    return out_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-15T21:30:49.264311Z",
     "start_time": "2019-05-15T21:30:48.609506Z"
    }
   },
   "outputs": [],
   "source": [
    "dict_bucketing = get_bucketing_allfeatures(df_train, final_features, n_bins=4)\n",
    "dict_bucketing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-15T21:30:50.002323Z",
     "start_time": "2019-05-15T21:30:49.981716Z"
    }
   },
   "outputs": [],
   "source": [
    "len(final_features), len(dict_bucketing.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-15T21:30:51.874287Z",
     "start_time": "2019-05-15T21:30:51.851120Z"
    }
   },
   "outputs": [],
   "source": [
    "def apply_bucketing(df, feat_col, cuts):\n",
    "    \"\"\"Applies a bucketing schema\n",
    "    \n",
    "    Args:\n",
    "        df: Pandas Dataframe with the input data\n",
    "        feat_col: Name of the column with the input feature\n",
    "        cuts: Cuts that will be applied to the input data\n",
    "        \n",
    "    Returns:\n",
    "        Pandas DataFrame with columns dummy columns\n",
    "    \"\"\"\n",
    "    if df[feat_col].dtypes in (object, bool):\n",
    "        df_buck = apply_bucketing_cat(df, feat_col, cuts)\n",
    "    else:\n",
    "        df_buck = apply_bucketing_num(df, feat_col, cuts)\n",
    "    return df_buck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-15T21:30:52.982076Z",
     "start_time": "2019-05-15T21:30:52.914430Z"
    }
   },
   "outputs": [],
   "source": [
    "apply_bucketing(df_train, 'CreditScoreRangeLower', dict_bucketing['CreditScoreRangeLower']).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-15T21:30:55.360172Z",
     "start_time": "2019-05-15T21:30:54.013602Z"
    }
   },
   "outputs": [],
   "source": [
    "# Apply the bucketing\n",
    "\n",
    "# Keep each column dummy columns in independent lists\n",
    "list_df_tr, list_df_val, list_df_oot = [], [], []\n",
    "for feat in final_features:\n",
    "    list_df_tr.append(apply_bucketing(df_train, feat, dict_bucketing[feat]))\n",
    "    list_df_val.append(apply_bucketing(df_val, feat, dict_bucketing[feat]))\n",
    "    list_df_oot.append(apply_bucketing(df_oot, feat, dict_bucketing[feat]))\n",
    "\n",
    "# Then 'vertically' combine them\n",
    "df_tr_preproc = pd.concat(list_df_tr, axis=1)\n",
    "df_val_preproc = pd.concat(list_df_val, axis=1)\n",
    "df_oot_preproc = pd.concat(list_df_oot, axis=1)\n",
    "\n",
    "# Capture the name of all buckets in our dataset\n",
    "keep_cols_buck = df_tr_preproc.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tr_preproc.shape, df_oot_preproc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-15T21:30:56.366797Z",
     "start_time": "2019-05-15T21:30:56.349901Z"
    }
   },
   "outputs": [],
   "source": [
    "# Capture the target in each subset\n",
    "y_tr, y_val, y_oot = df_train['bad'], df_val['bad'], df_oot['bad']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-15T21:31:04.519348Z",
     "start_time": "2019-05-15T21:31:04.314414Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-15T21:31:04.875031Z",
     "start_time": "2019-05-15T21:31:04.859449Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_auc(y_true, y_pred):\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(y_true, y_pred)\n",
    "    return metrics.auc(fpr, tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-15T21:31:07.303874Z",
     "start_time": "2019-05-15T21:31:06.535789Z"
    }
   },
   "outputs": [],
   "source": [
    "#check buckets correlations\n",
    "corr = df_tr_preproc[keep_cols_buck].corr()\n",
    "orig_features = keep_cols_buck.values.tolist()\n",
    "corr_TH = 0.75\n",
    "n_corr_list=[]\n",
    "corr_feats_list=[]\n",
    "for f in orig_features:\n",
    "    #get correlation entries for the feature\n",
    "    corr_f = corr[f][[col for col in orig_features if col!=f]]\n",
    "    #work with absolute value\n",
    "    corr_f_abs = corr_f.abs()\n",
    "    #get features above corr TH\n",
    "    corr_ht_th = corr_f_abs[corr_f_abs>corr_TH]\n",
    "    n_corr_list.append(corr_ht_th.shape[0])\n",
    "    corr_feats_list.append(corr_ht_th)\n",
    "\n",
    "corr_buckets = [(feat, n, feats_corr) for n, feats_corr, feat in zip(n_corr_list, corr_feats_list, orig_features) if n>0]\n",
    "corr_buckets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-15T21:31:07.807280Z",
     "start_time": "2019-05-15T21:31:07.782549Z"
    }
   },
   "outputs": [],
   "source": [
    "glm_cols = get_uncorr_feats(corr_buckets, orig_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glm_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-15T21:31:09.862098Z",
     "start_time": "2019-05-15T21:31:09.186937Z"
    }
   },
   "outputs": [],
   "source": [
    "#train first model\n",
    "lr = LogisticRegression(solver='lbfgs', C=0.01)\n",
    "lr.fit(df_tr_preproc[glm_cols], y_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tr_preproc[glm_cols].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-15T21:31:18.382052Z",
     "start_time": "2019-05-15T21:31:18.298663Z"
    }
   },
   "outputs": [],
   "source": [
    "# Predict for all subsets\n",
    "pred_log1_tr = lr.predict_proba(df_tr_preproc[glm_cols])[:, 1]\n",
    "pred_log1_val = lr.predict_proba(df_val_preproc[glm_cols])[:, 1]\n",
    "pred_log1_oot = lr.predict_proba(df_oot_preproc[glm_cols])[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-15T21:31:19.288402Z",
     "start_time": "2019-05-15T21:31:19.246779Z"
    }
   },
   "outputs": [],
   "source": [
    "Log1_aucs = get_auc(y_tr, pred_log1_tr), get_auc(y_val, pred_log1_val), get_auc(y_oot, pred_log1_oot)\n",
    "Log1_aucs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-15T21:31:21.003963Z",
     "start_time": "2019-05-15T21:31:20.959005Z"
    }
   },
   "outputs": [],
   "source": [
    "# Get p-values\n",
    "# Mostly from: https://gist.github.com/rspeare/77061e6e317896be29c6de9a85db301d\n",
    "\n",
    "import scipy.stats as stat\n",
    "def get_p_vals(lr, X):\n",
    "    denom = (2.0*(1.0+np.cosh(lr.decision_function(X))))\n",
    "    denom = np.tile(denom,(X.shape[1],1)).T\n",
    "    F_ij = np.dot((X/denom).T,X) ## Fisher Information Matrix\n",
    "    Cramer_Rao = np.linalg.inv(F_ij) ## Inverse Information Matrix\n",
    "    sigma_estimates = np.sqrt(np.diagonal(Cramer_Rao))\n",
    "    z_scores = lr.coef_[0]/sigma_estimates # z-score for eaach model coefficient\n",
    "    p_values = [stat.norm.sf(abs(x))*2 for x in z_scores] ### two tailed test for p-values#        \n",
    "    return p_values\n",
    "\n",
    "def show_lr_summary(p_values, features, lr):\n",
    "    df_ret = pd.DataFrame({'feature': features,\n",
    "                           'p_val': p_values,\n",
    "                           'betas': lr.coef_.tolist()[0]})[['feature', 'betas', 'p_val']]\n",
    "    return df_ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-15T21:31:26.263091Z",
     "start_time": "2019-05-15T21:31:25.754776Z"
    }
   },
   "outputs": [],
   "source": [
    "# If there was any issue with the data (high correlations...)\n",
    "# This would be messy, with NAs, or maybe not even able to run\n",
    "p_values = get_p_vals(lr, df_tr_preproc[glm_cols])\n",
    "show_lr_summary(p_values, glm_cols, lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### P-value based feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-15T21:32:20.370604Z",
     "start_time": "2019-05-15T21:31:43.047359Z"
    }
   },
   "outputs": [],
   "source": [
    "# remove each bucket with the highest p-value N times\n",
    "# assess how the AUC changes\n",
    "N_iterations = len(glm_cols) - 3\n",
    "glm_cols_pvals = [c for c in glm_cols]\n",
    "\n",
    "# keep the AUCs in each interation \n",
    "auc_train, auc_val, auc_oot = [], [], []\n",
    "# List of tuples where all selected features status will \n",
    "# be stored per iteration\n",
    "features_it_pval = []\n",
    "\n",
    "# for each iteration\n",
    "for it in range(N_iterations):\n",
    "    #capture the feature to be dropped\n",
    "    #in the first iteration we are using the p_values from the model trained 'outside'\n",
    "    feat_drop = [feat for feat, p in zip(glm_cols_pvals, p_values) if p == max(p_values)][0]\n",
    "    glm_cols_pvals.remove(feat_drop)\n",
    "    #re-train the model\n",
    "    lr_it = LogisticRegression(solver='lbfgs', C=0.01)\n",
    "    lr_it.fit(df_tr_preproc[glm_cols_pvals], y_tr)\n",
    "    # Predict for all subsets\n",
    "    pred_tr = lr_it.predict_proba(df_tr_preproc[glm_cols_pvals])[:, 1]\n",
    "    pred_val = lr_it.predict_proba(df_val_preproc[glm_cols_pvals])[:, 1]\n",
    "    pred_oot = lr_it.predict_proba(df_oot_preproc[glm_cols_pvals])[:, 1]\n",
    "    #get aucs\n",
    "    auc_train.append(get_auc(y_tr, pred_tr))\n",
    "    auc_val.append(get_auc(y_val, pred_val)) \n",
    "    auc_oot.append(get_auc(y_oot, pred_oot))\n",
    "    #get p-values\n",
    "    p_values = get_p_vals(lr_it, df_tr_preproc[glm_cols_pvals])\n",
    "    #keep features status at iteration\n",
    "    features_it_pval.append((it, [c for c in glm_cols_pvals]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-15T21:32:20.907606Z",
     "start_time": "2019-05-15T21:32:20.380909Z"
    }
   },
   "outputs": [],
   "source": [
    "# Plot AUCs in each iteration\n",
    "iterations = [i for i in range(N_iterations)]\n",
    "plt.plot(iterations, auc_train, label='Train')\n",
    "plt.plot(iterations, auc_val, label='Val')\n",
    "plt.plot(iterations, auc_oot, label='OOT')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-15T21:32:21.413331Z",
     "start_time": "2019-05-15T21:32:20.915613Z"
    }
   },
   "outputs": [],
   "source": [
    "# At iteration 40 more or less it stays stable\n",
    "final_features_bucketing = [feats for it, feats in features_it_pval if it==26][0]\n",
    "# Train the final model with the interesting buckets\n",
    "lr_final = LogisticRegression(solver='lbfgs', C=0.01)\n",
    "lr_final.fit(df_tr_preproc[final_features_bucketing], y_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-15T21:32:21.502326Z",
     "start_time": "2019-05-15T21:32:21.421221Z"
    }
   },
   "outputs": [],
   "source": [
    "# Predict for all subsets\n",
    "pred_tr = lr_final.predict_proba(df_tr_preproc[final_features_bucketing])[:, 1]\n",
    "pred_val = lr_final.predict_proba(df_val_preproc[final_features_bucketing])[:, 1]\n",
    "pred_oot = lr_final.predict_proba(df_oot_preproc[final_features_bucketing])[:, 1]\n",
    "\n",
    "Log2_aucs = get_auc(y_tr, pred_tr), get_auc(y_val, pred_val), get_auc(y_oot, pred_oot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-15T21:32:21.698146Z",
     "start_time": "2019-05-15T21:32:21.509227Z"
    }
   },
   "outputs": [],
   "source": [
    "p_values = get_p_vals(lr_final, df_tr_preproc[final_features_bucketing])\n",
    "show_lr_summary(p_values, final_features_bucketing, lr_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-15T10:56:52.402451Z",
     "start_time": "2019-05-15T10:56:52.390838Z"
    }
   },
   "source": [
    "#### AUC based feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-15T21:46:45.039235Z",
     "start_time": "2019-05-15T21:32:21.711350Z"
    }
   },
   "outputs": [],
   "source": [
    "# Try to remove all available features.\n",
    "# Remove the feature that has the lowest impact in AUC\n",
    "N_iterations = len(glm_cols) - 3\n",
    "glm_cols_auc = [c for c in glm_cols]\n",
    "\n",
    "auc_train, auc_val, auc_oot = [], [], []\n",
    "features_it_auc = []\n",
    "for it in range(N_iterations):\n",
    "    print('Working for iteration: {}'.format(str(it)))\n",
    "    #re-train the model\n",
    "    lr_it = LogisticRegression(solver='lbfgs', C=0.01)\n",
    "    lr_it.fit(df_tr_preproc[glm_cols_auc], y_tr)\n",
    "    # Predict for all subsets\n",
    "    pred_tr = lr_it.predict_proba(df_tr_preproc[glm_cols_auc])[:, 1]\n",
    "    pred_val = lr_it.predict_proba(df_val_preproc[glm_cols_auc])[:, 1]\n",
    "    pred_oot = lr_it.predict_proba(df_oot_preproc[glm_cols_auc])[:, 1]\n",
    "    #get base aucs\n",
    "    auc_train_it, auc_val_it, auc_oot_it = get_auc(y_tr, pred_tr), get_auc(y_val, pred_val), get_auc(y_oot, pred_oot)\n",
    "    auc_train.append(auc_train_it)\n",
    "    auc_val.append(auc_val_it) \n",
    "    auc_oot.append(auc_oot_it)\n",
    "    #set up minimum gap\n",
    "    min_gap = 500\n",
    "    for feat_eval in glm_cols_auc:\n",
    "        #use validation AUC only as evaluation metric\n",
    "        #keep features in iteration it, but the feature under evaluation\n",
    "        glm_cols_auc_ev = [c for c in glm_cols_auc if c!=feat_eval]\n",
    "        lr_it_ev = LogisticRegression(solver='lbfgs', C=0.01)\n",
    "        lr_it_ev.fit(df_tr_preproc[glm_cols_auc_ev], y_tr)\n",
    "        #predit @ val data\n",
    "        pred_val = lr_it_ev.predict_proba(df_val_preproc[glm_cols_auc_ev])[:, 1]\n",
    "        #get auc @ val\n",
    "        auc_val_it_ev = get_auc(y_val, pred_val)\n",
    "        #check gap\n",
    "        gap_val_auc = auc_val_it - auc_val_it_ev\n",
    "        #capture the feature that has the lowest AUC impact\n",
    "        if gap_val_auc < min_gap:\n",
    "            candidate_drop = feat_eval\n",
    "            min_gap = gap_val_auc\n",
    "    #remove from the feature set the selected feature\n",
    "    glm_cols_auc.remove(candidate_drop)\n",
    "    #keep features status at iteration\n",
    "    features_it_auc.append((it, [c for c in glm_cols_auc]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-15T21:46:45.585036Z",
     "start_time": "2019-05-15T21:46:45.049170Z"
    }
   },
   "outputs": [],
   "source": [
    "# Plot AUCs in each iteration\n",
    "iterations = [i for i in range(N_iterations)]\n",
    "plt.plot(iterations, auc_train, label='Train')\n",
    "plt.plot(iterations, auc_val, label='Val')\n",
    "plt.plot(iterations, auc_oot, label='OOT')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-15T21:46:45.889637Z",
     "start_time": "2019-05-15T21:46:45.592698Z"
    }
   },
   "outputs": [],
   "source": [
    "# get the iteration where it stays stable\n",
    "final_features_bucketing_auc = [feats for it, feats in features_it_auc if it==26][0]\n",
    "# Train the final model with the interesting buckets\n",
    "lr_final_auc = LogisticRegression(solver='lbfgs', C=0.01)\n",
    "lr_final_auc.fit(df_tr_preproc[final_features_bucketing_auc], y_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-15T21:46:45.967011Z",
     "start_time": "2019-05-15T21:46:45.897129Z"
    }
   },
   "outputs": [],
   "source": [
    "# Predict for all subsets\n",
    "pred_tr = lr_final_auc.predict_proba(df_tr_preproc[final_features_bucketing_auc])[:, 1]\n",
    "pred_val = lr_final_auc.predict_proba(df_val_preproc[final_features_bucketing_auc])[:, 1]\n",
    "pred_oot = lr_final_auc.predict_proba(df_oot_preproc[final_features_bucketing_auc])[:, 1]\n",
    "\n",
    "Log3_aucs = get_auc(y_tr, pred_tr), get_auc(y_val, pred_val), get_auc(y_oot, pred_oot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gain Table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO3** Implementa una función que te calcule una gain table, con una fila for bucket de scores, y con las siguientes columnas:\n",
    " - avg_pred: El score medio en ese bucket\n",
    " - BR: La tasa de malos en ese bucket\n",
    " - N_bads: El número de malos en ese bucket\n",
    " - N: El número de muestras en ese bucket\n",
    " - N_goods: El número de buenos en ese bucket\n",
    " - pct_bad_acum: El porcentaje de malos acumulados en ese bucket\n",
    " - pct_approv_acum: El porcentae de aprobación hasta ese bucket\n",
    " \n",
    "La tabla estará ordenada de scores más bajos (mejor perfil), a scores más altos (peor perfil)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tabla de eficiencia / Gain table\n",
    "def get_gain_table(pred, df, col_target='bad', n_buckets=10):\n",
    "    \"\"\"Generate the gain table given a population, and its predictions\n",
    "    \n",
    "    Args:\n",
    "        pred: np.array / pd.Series containing predictions\n",
    "        df: Pandas DataFrame containing the population to be assesed\n",
    "        col_target: Name of the target column\n",
    "        n_buckets: Number of buckets for the gain table\n",
    "        \n",
    "    Returns:\n",
    "        Pandas DataFrame representing the gain table\n",
    "    \"\"\"\n",
    "    df['pred'] = pred\n",
    "    pred_cuts = df['pred'].quantile(np.linspace(0, 1, num = n_buckets + 1))\n",
    "    df['pred_cut'] = # Use the pd.cut() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# usa la función get_gain_table en el conjunto oot\n",
    "get_gain_table(pred_oot, df_oot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO 4** Comenta la gain table anterior (sobre el conjunto OOT). ¿Qué conclusiones podemos sacar con respecto al funcionamiento del modelo? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Introduce tu comentario aquí."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gain table for training sample\n",
    "get_gain_table(pred_tr, df_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-15T21:46:46.359824Z",
     "start_time": "2019-05-15T21:46:46.300838Z"
    }
   },
   "outputs": [],
   "source": [
    "# Lighther preprocessing\n",
    "# Handle NAs @ numeric features\n",
    "# fill with a value lower than its minimum\n",
    "# We can reuse previopus categorical preprocessing\n",
    "\n",
    "dtypes = df_train[final_features].dtypes\n",
    "cat_features = df_train[features].select_dtypes(include=[\"bool\", \"object\"]).columns.tolist()\n",
    "num_feats = [f for f in final_features if f not in cat_features]\n",
    "\n",
    "def get_nafill_rf_num(df, num_features, gap_min=1e6):\n",
    "    \"\"\"Get a dictionary, that will store the value that will be used \n",
    "    to fill NAs in numeric data.\n",
    "    \n",
    "    Args:\n",
    "        df: Pandas Dataframe with the input data\n",
    "        num_features: List with the names of categorical features\n",
    "        gap_min: Gap between minimum value and filling value\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary, with the following structure:\n",
    "        {feature1: fill_val1,\n",
    "         feature2: fill_val2}\n",
    "    \"\"\"\n",
    "    dict_fill = {}\n",
    "    for num_feat in num_features:\n",
    "        dict_fill[num_feat] = df[num_feat].min() - gap_min\n",
    "    return dict_fill\n",
    "    \n",
    "\n",
    "def apply_nafill_rf_num(df, dict_fillrf):\n",
    "    \"\"\"Given a dictionary with the values to be used in NA filling,\n",
    "    use it to fill NAs.\n",
    "    \n",
    "    Args:\n",
    "        df: Pandas Dataframe with the input data\n",
    "        dict_fillrf: Dictionary, that stores filling values\n",
    "        \n",
    "    Returns:\n",
    "        Pandas Dataframe with NA being filled\n",
    "    \"\"\"\n",
    "    df_out = df.copy()\n",
    "    for num_feat in dict_fillrf.keys():\n",
    "        df_out.loc[df_out[num_feat].isna(), num_feat] = dict_fillrf[num_feat]\n",
    "    return df_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-15T21:46:47.050672Z",
     "start_time": "2019-05-15T21:46:46.369092Z"
    }
   },
   "outputs": [],
   "source": [
    "#get dictionary to fill values from train\n",
    "dict_nafill = get_nafill_rf_num(df_train, num_feats)\n",
    "#apply it to train, val and oot\n",
    "df_train_fill = apply_nafill_rf_num(df_train, dict_nafill)\n",
    "df_val_fill = apply_nafill_rf_num(df_val, dict_nafill)\n",
    "df_oot_fill = apply_nafill_rf_num(df_oot, dict_nafill)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-15T21:46:47.272707Z",
     "start_time": "2019-05-15T21:46:47.056863Z"
    }
   },
   "outputs": [],
   "source": [
    "# Keep each column dummy columns in independednt lists\n",
    "# Initialize with the numeric data already filled\n",
    "list_df_tr, list_df_val, list_df_oot = [df_train_fill[num_feats]], [df_val_fill[num_feats]], [df_oot_fill[num_feats]]\n",
    "# Add the bucketing results of categorical data\n",
    "for feat in cat_features:\n",
    "    list_df_tr.append(apply_bucketing(df_train, feat, dict_bucketing[feat]))\n",
    "    list_df_val.append(apply_bucketing(df_val, feat, dict_bucketing[feat]))\n",
    "    list_df_oot.append(apply_bucketing(df_oot, feat, dict_bucketing[feat]))\n",
    "\n",
    "# Then combine them column-wise\n",
    "df_tr_preproc_rf = pd.concat(list_df_tr, axis=1)\n",
    "df_val_preproc_rf = pd.concat(list_df_val, axis=1)\n",
    "df_oot_preproc_rf = pd.concat(list_df_oot, axis=1)\n",
    "\n",
    "# Keep the final column names\n",
    "keep_cols_rf = df_tr_preproc_rf.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO 5** Entrena un RandomForestClassifier, y evalua su rendimiento (AUC, gain table). Revisa la documentación del modelo, y elige parámetros razonables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a RF-Classifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "X_tr = df_tr_preproc_rf[keep_cols_rf]\n",
    "X_val = df_val_preproc_rf[keep_cols_rf]\n",
    "X_oot = df_oot_preproc_rf[keep_cols_rf]\n",
    "\n",
    "# Train your model\n",
    "rf = # rf is the RandomForestClassifier object you will work with\n",
    "\n",
    "# Predict for all subsets\n",
    "pred_rf_tr = #\n",
    "pred_rf_val = # \n",
    "pred_rf_oot = #\n",
    "\n",
    "# Get AUC metrics in all subsets\n",
    "                       \n",
    "# Compute the gain table for the OOT set\n",
    "rf_aucs = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-15T21:46:57.220867Z",
     "start_time": "2019-05-15T21:46:56.322066Z"
    }
   },
   "outputs": [],
   "source": [
    "# Show Feature importance\n",
    "imp_df = pd.DataFrame({'feature': keep_cols_rf,\n",
    "                       'importance': rf.feature_importances_})\n",
    "imp_df.sort_values(by='importance', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO 6** Entrena un XGBClassifier, y evalua su rendimiento (AUC, gain table). Revisa la documentación del modelo y elige parámetros razonables https://xgboost.readthedocs.io/en/stable/python/python_api.html#xgboost.XGBClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a XGBClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Train your model\n",
    "xgb = # xgb is the  object you will work with\n",
    "\n",
    "# Predict for all subsets\n",
    "pred_xgb_tr = #\n",
    "pred_xgb_val = # \n",
    "pred_xgb_oot = #\n",
    "# Get AUC metrics in all subsets\n",
    "                       \n",
    "                          \n",
    "# Compute the gain table for the OOT set\n",
    "xgb_aucs = # ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sow feature importance\n",
    "imp_df = pd.DataFrame({'feature': keep_cols_rf,\n",
    "                       'importance': xgb.feature_importances_})\n",
    "imp_df.sort_values(by='importance', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare model performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_aucs = pd.DataFrame([Log1_aucs, Log2_aucs, Log3_aucs, rf_aucs, xgb_aucs])\n",
    "df_aucs.columns = [\"AUC_Train\", \"AUC_Val\", \"AUC_OOT\"]\n",
    "df_aucs.index = [\"Logistic\", \"Log_Pval\", \"Log_AUC\", \"RF\", \"XGBoost\"]\n",
    "df_aucs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO 7** Comenta el rendimiento de los modelos. ¿Qué modelo moverias a producción? "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
