{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Miembros del grupo:\n",
    "- \n",
    "- \n",
    "-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-15T18:44:28.074231Z",
     "start_time": "2019-04-15T18:44:26.402396Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-15T18:44:28.458976Z",
     "start_time": "2019-04-15T18:44:28.080437Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/prosperLoanData_train.csv')\n",
    "df_oot = pd.read_csv('../data/prosperLoanData_oot.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-15T18:44:28.512408Z",
     "start_time": "2019-04-15T18:44:28.465199Z"
    }
   },
   "outputs": [],
   "source": [
    "df.groupby('bad').size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Differenciate between usable features and other columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-15T18:44:28.613602Z",
     "start_time": "2019-04-15T18:44:28.521902Z"
    }
   },
   "outputs": [],
   "source": [
    "df_columns = pd.read_excel(\"../prac4/Clasificacion_columnas_sol.xlsx\")\n",
    "drop = df_columns.loc[df_columns[\"Keep/Drop\"]==\"Drop\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-15T18:44:28.665514Z",
     "start_time": "2019-04-15T18:44:28.639849Z"
    }
   },
   "outputs": [],
   "source": [
    "features = [c for c in df.columns if c not in drop.Column.values]\n",
    "col_target = 'bad'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El primer enfoque que le daremos a nuestras variables será el conocer el tipo de variables que son.\n",
    "- Por una parte, observamos que muchas variables son de tipo *float64*, las cuales serán como **variables numéricas**\n",
    "- Por otra parte, observamos que otras variables son de tipo *object* o incluso *booleanos*, las cuales serán **variables categóricas**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-15T18:44:28.752114Z",
     "start_time": "2019-04-15T18:44:28.676040Z"
    }
   },
   "outputs": [],
   "source": [
    "df[features].dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Establecemos una disgregación de variables, para ello separemos aquellas variables que sean categóricas de las que sean numéricas y sobre ellas realizaremos un análisis estadístico descriptivo para ir comparándo unas con las otras e intrepetar los resultados estadísticamente más significativos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-15T18:44:28.797661Z",
     "start_time": "2019-04-15T18:44:28.763459Z"
    }
   },
   "outputs": [],
   "source": [
    "cat_features = [f for f, b in zip(features, df[features].dtypes == object) if b==True]\n",
    "num_features = [f for f in features if f not in cat_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-15T18:44:29.218823Z",
     "start_time": "2019-04-15T18:44:28.808634Z"
    }
   },
   "outputs": [],
   "source": [
    "df[num_features].describe().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-15T18:44:29.335400Z",
     "start_time": "2019-04-15T18:44:29.227260Z"
    }
   },
   "outputs": [],
   "source": [
    "df[cat_features].describe().transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Pandas describing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO 1** Crea una función que cumpla con la funcionalidad descrita en la cabecera de la función desc_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-15T18:44:29.591738Z",
     "start_time": "2019-04-15T18:44:29.512379Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def desc_num(df, df_oot, col):\n",
    "    \"\"\"Function that returns a custom descriptive for the numerical variable. It returns:\n",
    "        - Mean, median, minimum, maximum, p25, p75, std, %na %nonzero, %unique\n",
    "        - Stability plot\n",
    "    \n",
    "    Args:\n",
    "        df: Pandas DataFrame with the in time input data\n",
    "        df_oot: Pandas DataFrame with the out of time input data\n",
    "        col: Name of the column with the feature under study\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary that contains the main statistics of the feature, \n",
    "        also shows both the histogram and stability plots\n",
    "    \"\"\"\n",
    "    \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-15T18:44:59.163819Z",
     "start_time": "2019-04-15T18:44:29.620535Z"
    }
   },
   "outputs": [],
   "source": [
    "#### EJECUTA AQUÍ LA FUNCIÓN desc_num para todas la variables numéricas\n",
    "for feat in num_features:\n",
    "    print(f\"Results for {feat}\")\n",
    "    display(desc_num(df, df_oot, feat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO 2** Crea una función que cumpla con la funcionalidad descrita en la cabecera de la función desc_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-15T18:44:59.211251Z",
     "start_time": "2019-04-15T18:44:59.170057Z"
    }
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def desc_cat(df, df_oot, col):\n",
    "    \"\"\"Function that returns a custom descriptive for the categorical variable. It returns:\n",
    "        - # Unique entires, % Unique entries, top frequent entry, frequency of top, NA rate\n",
    "        - Stability plot\n",
    "    \n",
    "    Args:\n",
    "        df: Pandas DataFrame with the in time input data\n",
    "        df_oot: Pandas DataFrame with the out of time input data\n",
    "        col: Name of the column with the feature under study\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary that contains the main statistics of the feature, \n",
    "        also shows both the histogram and stability plots\n",
    "    \"\"\"\n",
    "    \n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-15T18:45:02.405998Z",
     "start_time": "2019-04-15T18:44:59.217114Z"
    }
   },
   "outputs": [],
   "source": [
    "#### EJECUTA AQUÍ LA FUNCIÓN desc_cat para todas las variable categóricas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation Review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-15T18:45:04.516390Z",
     "start_time": "2019-04-15T18:45:02.411359Z"
    }
   },
   "outputs": [],
   "source": [
    "corr = df[num_features].corr()\n",
    "mask = np.zeros_like(corr)\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "f, ax = plt.subplots(figsize=(11, 9))\n",
    "cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "sns.heatmap(corr, mask=mask, cmap=cmap, vmax=1., center=0,\n",
    "            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-15T18:45:04.564392Z",
     "start_time": "2019-04-15T18:45:04.525922Z"
    }
   },
   "outputs": [],
   "source": [
    "corr[corr!=1].max().sort_values().tail(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get pairs of highly correlated features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-15T18:45:04.587383Z",
     "start_time": "2019-04-15T18:45:04.574521Z"
    }
   },
   "outputs": [],
   "source": [
    "corr_TH = 0.75\n",
    "n_corr_list = []\n",
    "corr_feats_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-15T18:45:04.804682Z",
     "start_time": "2019-04-15T18:45:04.597583Z"
    }
   },
   "outputs": [],
   "source": [
    "for f in num_features:\n",
    "    corr_f = corr[f][[col for col in num_features if col!=f]]\n",
    "    corr_f_abs = corr_f.abs()\n",
    "    corr_ht_th = corr_f_abs[corr_f_abs>corr_TH]\n",
    "    n_corr_list.append(corr_ht_th.shape[0])\n",
    "    corr_feats_list.append(corr_ht_th)\n",
    "    \n",
    "corr_relations = [(feat, n, feats_corr) for n, feats_corr, feat in zip(n_corr_list, corr_feats_list, num_features) if n>0]\n",
    "corr_relations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recommended tool: pandas-profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-15T18:45:13.574210Z",
     "start_time": "2019-04-15T18:45:04.813264Z"
    }
   },
   "outputs": [],
   "source": [
    "from ydata_profiling import ProfileReport\n",
    "profile = ProfileReport(df[features])\n",
    "profile.to_file(output_file='output.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(profile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IV / WoE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez identificadas las variables más importantes, el siguiente paso que tenemos que hacer es establecer los trameados (buckets), con la misma acción para las variables categóricas y numéricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-15T18:45:13.971662Z",
     "start_time": "2019-04-15T18:45:13.764444Z"
    }
   },
   "outputs": [],
   "source": [
    "#visualization functions\n",
    "def capture_df(feat_col, input_slider, n_bins, df, target_col):\n",
    "    \"\"\"\n",
    "    Handles the type of the data to generate the intermediate datadframe\n",
    "    \"\"\"\n",
    "    if df[feat_col].dtype in [int, float, np.number]:\n",
    "        return df_vol_br_num(feat_col, input_slider, n_bins, df, target_col)\n",
    "    else:\n",
    "        return df_vol_br_cat(feat_col, input_slider, n_bins, df, target_col)\n",
    "\n",
    "#capture volume / BR df for numerical variables\n",
    "def df_vol_br_num(feat_col, input_slider, n_bins, df, obj_col):\n",
    "    \"\"\"\n",
    "    Generate the intermediate dataframe with number of observations and \n",
    "    number of bads per bin. Specific for numerical features.\n",
    "    \"\"\"\n",
    "    #get the numeric input from the dual slider\n",
    "    perc_sliders = [v/100. for v in input_slider]\n",
    "    var_lims = df[feat_col].quantile([perc_sliders[0], perc_sliders[1]]).values\n",
    "    v_min, v_max = var_lims[0], var_lims[1]\n",
    "    #filter the dataset using the slider input\n",
    "    df_cut = df.loc[(df[feat_col] <= v_max) & (df[feat_col] >= v_min)][[obj_col, feat_col]]\n",
    "    #number of cuts = minumum of n_bins, number of unique values of the variable\n",
    "    n_cuts = min(int(n_bins), df_cut[feat_col].nunique())\n",
    "    cuts = [c for c in np.linspace(v_min, v_max, n_cuts + 1)]\n",
    "    if cuts[-1] < v_max:\n",
    "        cuts.append(v_max)\n",
    "    cut_col = feat_col + '_'\n",
    "    df_cut[cut_col] = pd.cut(df_cut[feat_col], cuts, include_lowest=True)\n",
    "    #generate aggregated values\n",
    "    N = df_cut.groupby(cut_col)[feat_col].count().values\n",
    "    TR = df_cut.groupby(cut_col)[obj_col].mean().values\n",
    "    cuts = df_cut.groupby(cut_col)[feat_col].count().index.astype(str).values\n",
    "    #handle NA entries\n",
    "    if df[feat_col].isna().sum() > 0:\n",
    "        N = np.append(([df[feat_col].isna().sum()]), N)\n",
    "        TR = np.append(([df.loc[df[feat_col].isna()][obj_col].mean()]), TR)\n",
    "        cuts =  np.append(['NA'], cuts)\n",
    "    #generate global transformation rate\n",
    "    return (pd.DataFrame({'cuts': cuts,\n",
    "                         'N': N,\n",
    "                         'BR': TR}), df_cut[obj_col].mean())\n",
    "\n",
    "#capture volume / BR df for categorical variables\n",
    "def df_vol_br_cat(feat_col, input_slider, n_bins, df, target_col):\n",
    "    \"\"\"\n",
    "    Generate the intermediate dataframe with number of observations and \n",
    "    number of bads per bin. Specific for categorical features.\n",
    "    \"\"\"\n",
    "    #pick top n_bins levels by volume\n",
    "    cut_levels = df.groupby(feat_col)[feat_col].count().sort_values(ascending=False)[:int(n_bins)].index.values.tolist()\n",
    "    df_cut = df.loc[df[feat_col].isin(cut_levels)]\n",
    "    #capture volumes\n",
    "    N = df_cut.groupby(feat_col)[feat_col].count().values\n",
    "    #capture transformations\n",
    "    TR = df_cut.groupby(feat_col)[target_col].mean().values\n",
    "    return (pd.DataFrame({'cuts': df_cut.groupby(feat_col)[feat_col].count().index.astype(str).values,\n",
    "                         'N': N,\n",
    "                         'BR': TR}), df_cut[target_col].mean())\n",
    "\n",
    "\n",
    "def output_graph_update(feat_col, input_slider, n_bins, df, obj_col):\n",
    "    \"\"\"\n",
    "    Generate the plotly plot showing the visualization of the intermediate \n",
    "    dataframe with volume and bad rate per bin.\n",
    "    \"\"\"\n",
    "    #get the df with volume and bad rate\n",
    "    df_tr, avg_tr = capture_df(feat_col, input_slider, n_bins, df, obj_col)\n",
    "    #line represents transformation rate\n",
    "    tr_line = go.Scatter(x = df_tr.cuts,\n",
    "                         y = df_tr.BR,\n",
    "                         yaxis = 'y2',\n",
    "                         name = 'BR')\n",
    "    #bar represents volume @ cut\n",
    "    vol_bars = go.Bar(x = df_tr.cuts,\n",
    "                      y = df_tr.N,\n",
    "                      name = 'Volume')\n",
    "    #avg line\n",
    "    avg_line = go.Scatter(x = df_tr.cuts,\n",
    "                          y = np.repeat(avg_tr, df_tr.shape[0]),\n",
    "                          yaxis = 'y2',\n",
    "                          name = 'AVG BR',\n",
    "                          line = dict(\n",
    "                              color = ('rgb(205, 0, 0)')\n",
    "                                     )\n",
    "                         )\n",
    "    #small layout\n",
    "    layout = go.Layout(\n",
    "            title = 'BR for ' + feat_col,\n",
    "            yaxis = dict(title = 'Volume',\n",
    "                         range = [0, max(df_tr.N)]),\n",
    "            yaxis2 = dict(title = 'BR',\n",
    "                         overlaying='y',\n",
    "                         side='right',\n",
    "                         range = [0, max(df_tr.BR) + 0.05*max(df_tr.BR)])\n",
    "\n",
    "        )\n",
    "    return {'data': [vol_bars, tr_line, avg_line],\n",
    "            'layout': layout}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo de una variable cortada en buckets\n",
    "df_buckets, avg_ratio = df_vol_br_num('CreditScoreRangeLower', [0, 95], 5, df,'bad')\n",
    "df_buckets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO 3** Implementa una función que calcule el IV de una variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_IV(df_buckets, col_vol=\"N\", col_BR=\"BR\"):\n",
    "    \"\"\"Function that estimates the IV of a variable. It returns:\n",
    "        - IV estimation (float)\n",
    "    \n",
    "    Args:\n",
    "        df_agg: Pandas DataFrame with the bucketed volume and bad rate\n",
    "        col_vol: Name of the column with the volume per bucket\n",
    "        col_BR: Name of the column with the bad rate per bucket\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary that contains the main statistics of the feature\n",
    "    \"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Estimate the IV for all features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-15T18:45:17.432748Z",
     "start_time": "2019-04-15T18:45:14.067597Z"
    }
   },
   "outputs": [],
   "source": [
    "ivs = []\n",
    "for c in features:\n",
    "    df_tr, avg_br = capture_df(c, [0., 95.], 5, df, 'bad')\n",
    "    ivs.append(get_IV(df_tr))\n",
    "df_iv = pd.DataFrame({'feature': features,\n",
    "                      'IV': ivs}).sort_values(by='IV', ascending=False)\n",
    "df_iv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize some features versus the Bad rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-15T18:45:18.245267Z",
     "start_time": "2019-04-15T18:45:17.442958Z"
    }
   },
   "outputs": [],
   "source": [
    "#dynamic plotting libraries\n",
    "import plotly.offline as py\n",
    "import plotly.graph_objs as go\n",
    "py.init_notebook_mode(connected=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_iv.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-15T18:45:21.475950Z",
     "start_time": "2019-04-15T18:45:18.305382Z"
    }
   },
   "outputs": [],
   "source": [
    "#plot features\n",
    "for c in df_iv.sort_values(by=\"IV\", ascending=False).feature.values.tolist():\n",
    "    py.iplot(output_graph_update(c, [0., 97.5], 6, df, 'bad'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stability check - PSI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EL **PSI** es el test de estabilidad de la población. Más sobre el PSI en [esta web](https://mwburke.github.io/data%20science/2018/04/29/population-stability-index.html). El umbral de valores extraídos del PSI se interpreta de la siguiente manera:\n",
    "\n",
    "\n",
    "- PSI <= 0,10 sin cambios significativos\n",
    "-  0,10 < PSI <= 0,25 pequeños cambios,  investigar\n",
    "- PSI > 0,25 cambio significativo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-15T18:45:21.560269Z",
     "start_time": "2019-04-15T18:45:21.482272Z"
    }
   },
   "outputs": [],
   "source": [
    "def PSI_numeric(series, in_out_time_series):\n",
    "    \"\"\"Returns the population stability index for numerical variables\n",
    "    \n",
    "    Args:\n",
    "        series: Pandas Series, the variable to describe\n",
    "        in_out_time_series: Pandas Series It contains the in time / out of time series\n",
    "        \n",
    "    Returns:\n",
    "        Estimated PSI\n",
    "    \"\"\"\n",
    "    pd_aux = pd.DataFrame(dict(data = series, in_out = in_out_time_series)).reset_index()\n",
    "    #capture in time and out of time series\n",
    "    in_series = pd_aux.loc[pd_aux.in_out == True]['data']\n",
    "    out_series = pd_aux.loc[pd_aux.in_out == False]['data']\n",
    "\n",
    "    #base data deciles\n",
    "    qqs = in_series.quantile(q=[0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1])\n",
    "\n",
    "    #cut the data, based in the base series deciles\n",
    "    in_series_cut = pd.cut(in_series, sorted(list(set(qqs.values))), include_lowest=True)\n",
    "    out_series_cut = pd.cut(out_series, sorted(list(set(qqs.values))), include_lowest=True)\n",
    "    #count volume per bin\n",
    "    in_grp = in_series_cut.value_counts(dropna=False)\n",
    "    out_grp = out_series_cut.value_counts(dropna= not (np.nan in in_grp.index.values.tolist()))\n",
    "    #small fix, so some inf values are fixed\n",
    "    out_grp[out_grp==0] = 0.01\n",
    "\n",
    "    #N observations in each series\n",
    "    N_in = len(in_series_cut)\n",
    "    N_out = len(out_series_cut)\n",
    "\n",
    "    #convert to share in each bin\n",
    "    in_grp = in_grp / N_in\n",
    "    out_grp = out_grp / N_out\n",
    "\n",
    "    return sum((in_grp-out_grp)*np.log(in_grp/out_grp))\n",
    "\n",
    "def PSI_categorical(series, in_out_time_series):\n",
    "    \"\"\"Returns the population stability index for categorical variables\n",
    "    \n",
    "    Args:\n",
    "        series: Pandas Series, the variable to describe\n",
    "        in_out_time_series: Pandas Series It contains the in time / out of time series\n",
    "        \n",
    "    Returns:\n",
    "        Estimated PSI\n",
    "    \"\"\"\n",
    "    pd_aux = pd.DataFrame(dict(data = series, in_out = in_out_time_series)).reset_index()\n",
    "    #capture in time and out of time series\n",
    "    in_series = pd_aux.loc[pd_aux.in_out == True]['data']\n",
    "    out_series = pd_aux.loc[pd_aux.in_out == False]['data']\n",
    "    \n",
    "    #count volume per level\n",
    "    in_grp = in_series.value_counts(dropna=False)\n",
    "    out_grp = out_series.value_counts(dropna= not (np.nan in in_grp.index.values.tolist()))\n",
    "    \n",
    "    #N observations in each series\n",
    "    N_in = len(in_series)\n",
    "    N_out = len(out_series)\n",
    "    \n",
    "    #convert to share in each bin\n",
    "    in_grp = in_grp / N_in\n",
    "    out_grp = out_grp / N_out\n",
    "    \n",
    "    #put all together in a df\n",
    "    df_grp = in_grp.to_frame().join(out_grp.to_frame(), lsuffix = '_in', rsuffix = '_out')\n",
    "    df_grp = df_grp.fillna(0.000001)\n",
    "\n",
    "    return sum((df_grp.data_in - df_grp.data_out) * np.log(df_grp.data_in / df_grp.data_out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-15T18:45:22.882811Z",
     "start_time": "2019-04-15T18:45:21.565084Z"
    }
   },
   "outputs": [],
   "source": [
    "psi = []\n",
    "#capture in time - out of time series\n",
    "it_oot_series = pd.Series(np.hstack((np.ones(len(df)), np.zeros(len(df_oot)))))\n",
    "#for all features\n",
    "\n",
    "for c in features:\n",
    "    col_series = pd.concat([df[c], df_oot[c]], ignore_index=True)\n",
    "    if df[c].dtypes == object:\n",
    "        psi.append(PSI_categorical(col_series, it_oot_series))\n",
    "    else:\n",
    "        psi.append(PSI_numeric(col_series, it_oot_series))\n",
    "\n",
    "df_psi = pd.DataFrame({'feature': features,\n",
    "                       'PSI': psi})\n",
    "df_psi = df_psi.sort_values(by='PSI')\n",
    "df_psi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO 4** Selecciona las dos variables más estables, y procesalas con la función \"desc_num\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO 5** Selecciona las dos variables menos estables, y procesalas con la función \"desc_num\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-15T18:45:37.722349Z",
     "start_time": "2019-04-15T18:45:37.680641Z"
    }
   },
   "outputs": [],
   "source": [
    "#save the data, as it will come handy in future work\n",
    "# Se escribe serializado\n",
    "import pickle\n",
    "#build a dictionary and serialize it\n",
    "dict_dump = {'features': features,\n",
    "             'corr_data': corr_relations,\n",
    "             'iv_df': df_iv,\n",
    "             'psi_df': df_psi}\n",
    "pickle.dump(dict_dump, open('output_HW5.pkl', 'wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
